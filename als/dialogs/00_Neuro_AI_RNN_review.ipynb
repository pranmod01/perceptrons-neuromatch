{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["goal\n", "- translate this code to train locally\n", "- make the net learn correctly\n", "- make tests"], "attachments": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["```\n", "\n", "import numpy as np\n", "\n", "# Scikit-Learn for machine learning utilities\n", "from sklearn.decomposition import PCA\n", "from sklearn import manifold\n", "\n", "# --- Plotting tools\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "\n", "# --- Torch tools for the RNN\n", "import torch\n", "import torch.nn as nn\n", "from torch.utils.data import Dataset, DataLoader\n", "\n", "# @title Loads RDMs data from google drive\n", "\n", "loaded = np.load('/content/drive/MyDrive/Colab Notebooks/Neuromatch/NeuroAI/Project/rdms_by_layer.npz')\n", "\n", "# List all layers stored\n", "print(\"Layers saved:\", loaded.files)\n", "\n", "# Access a specific RDM, e.g. for 'softmax'\n", "softmax_rdm = loaded['softmax']\n", "\n", "\n", "# @title Applies MDS on RDMs to reduce dimensionality\n", "\n", "def dim_reduction(model_features, transformation_funcs, n_components=2):\n", "    \"\"\"\n", "    Reduce dimensionality for model features using various transformations.\n", "\n", "    Inputs:\n", "    - model_features (dict): Dictionary containing model features for each layer.\n", "    - transformer_funcs (list): List of dimensionality reduction techniques to apply ('PCA', 'MDS', 't-SNE').\n", "    \"\"\"\n", "    transformations = []\n", "    reduced = {}\n", "    # --- Selects method for dimensionality reduction\n", "    for t in transformation_funcs:\n", "        if t == 'PCA': transformations.append(PCA(n_components=n_components))\n", "        if t == 'MDS': transformations.append(manifold.MDS(n_components=n_components, normalized_stress='auto'))\n", "        if t == 't-SNE': transformations.append(manifold.TSNE(n_components=n_components, perplexity=40, verbose=0))\n", "\n", "    return_layers = list(model_features.keys())\n", "\n", "    # --- Apply selected method for dimensionality reduction\n", "    for f in range(len(transformation_funcs)):\n", "        # --- Reduces dimensionality for each layer in the list\n", "        for l in range(len(return_layers)):\n", "            layer =  return_layers[l]\n", "            feats = model_features[layer]\n", "            feats_transformed= transformations[f].fit_transform(feats)\n", "            reduced[layer] = feats_transformed\n", "\n", "    return reduced\n", "\n", "model_features = loaded # Here is where RDMs are feeded to the MDS dimensionality reduction\n", "# rdms_dict should be a dictionary with layers as keys, e.g. {'conv1': rmds_array}\n", "# --- List of methods to use\n", "transformation_funcs =['MDS']\n", "# --- Returns reduced rdms\n", "reduced_features = dim_reduction(model_features, transformation_funcs, n_components=64)\n", "\n", "\n", "\n", "# @title Plot reduced RDMs dfor the first 3 digits\n", "\n", "fig, axes = plt.subplots(1, 4, figsize=(24, 5)) # Create a figure with 1 row and 4 columns of subplots\n", "\n", "# Plot the first heatmap\n", "sns.heatmap(reduced_features['softmax'].T[:,0:30], cmap='viridis', ax=axes[0])\n", "axes[0].set_title(\"Reduced Features \u2013 '0000000000'\")\n", "axes[0].set_xlabel(\"Stimuli\")\n", "axes[0].set_ylabel(\"Stimuli\")\n", "\n", "# Plot the second heatmap (replace X_train with your data for the second plot)\n", "sns.heatmap(reduced_features['softmax'].T[:,30:60], cmap='viridis', ax=axes[1])\n", "axes[1].set_title(\"Reduced Features \u2013 '1000000000'\")\n", "axes[1].set_xlabel(\"Stimuli\")\n", "axes[1].set_ylabel(\"Stimuli\")\n", "\n", "# Plot the third heatmap (replace X_train with your data for the third plot)\n", "sns.heatmap(reduced_features['softmax'].T[:,60:90], cmap='viridis', ax=axes[2])\n", "axes[2].set_title(\"Reduced Features \u2013 '0100000000'\")\n", "axes[2].set_xlabel(\"Stimuli\")\n", "axes[2].set_ylabel(\"Stimuli\")\n", "\n", "# Plot the fourth heatmap (replace X_train with your data for the fourth plot)\n", "sns.heatmap(reduced_features['softmax'].T[:,90:120], cmap='viridis', ax=axes[3])\n", "axes[3].set_title(\"Reduced Features \u2013 '0010000000'\")\n", "axes[3].set_xlabel(\"Stimuli\")\n", "axes[3].set_ylabel(\"Stimuli\")\n", "\n", "plt.tight_layout() # Adjust layout to prevent overlapping titles\n", "plt.show()\n", "\n", "\n", "# @title Create the dataset form the reduced RDMs\n", "\n", "def build_dataset(reduced_layer_features):\n", "    \"\"\"\n", "    Build a dataset from reduced features.\n", "    \"\"\"\n", "\n", "    # -- MNIST digits\n", "    digits = 10\n", "    # --- Since there is 30 stimulus for each digit, use 25 for trainning and 5 for test\n", "    train_n_samples = 25\n", "    test_n_samples = 5\n", "    # --- Samples noise from uniform distribution for the (non stimulus) intervals\n", "    noise_train = np.random.uniform(0.0, 1e-2, (64, train_n_samples))\n", "    noise_test = np.random.uniform(0.0, 1e-2, (64, test_n_samples))\n", "    # --- Creates base vectors for the one hot encoding labels\n", "    one_hot_encode = np.zeros((10,1))\n", "    one_hot_encode_noise = np.ones((10,1))\n", "\n", "    # --- Creates matrices for training and test datasets\n", "    X_train = reduced_layer_features[:, 0:train_n_samples]\n", "    X_test = reduced_layer_features[:, train_n_samples:train_n_samples + test_n_samples]\n", "    # --- Creates the label matrices for training and test\n", "    one_hot_encode[0, :] = 1\n", "    y_train = np.tile(one_hot_encode, (1, train_n_samples))\n", "    y_test = np.tile(one_hot_encode, (1, test_n_samples))\n", "    # --- Takes base vector back to all zeros\n", "    one_hot_encode[0, :] = 0\n", "\n", "    # --- Iterates over the digits\n", "    for digit in range(1, digits):\n", "\n", "        # --- Each 30 stimulus the digit has changed on the RDMs\n", "        idx_start = 30 * digit\n", "        idx_end = (30 * digit) + train_n_samples\n", "\n", "        # --- Adds noise interval (no stimulus) to training and test datasets\n", "        X_train = np.concatenate((X_train, noise_train), axis=1)\n", "        X_test = np.concatenate((X_test, noise_train), axis=1)\n", "        # --- Adds noise interval (no stimulus) to training and test labels\n", "        y_train = np.concatenate((y_train, np.tile(one_hot_encode_noise, (1, train_n_samples))), axis=1)\n", "        y_test = np.concatenate((y_test, np.tile(one_hot_encode_noise, (1, train_n_samples))), axis=1)\n", "\n", "        # --- Concatenates current digit data to training and test datasets\n", "        X_train = np.concatenate((X_train, reduced_layer_features[:, idx_start:idx_end]), axis=1)\n", "        X_test = np.concatenate((X_test, reduced_layer_features[:, idx_end:idx_end + test_n_samples]), axis=1)\n", "        # --- Defines one hot encoding for current digit\n", "        one_hot_encode[digit, :] = 1\n", "        # --- Concatenates current digit data to training and test labels\n", "        y_train = np.concatenate((y_train, np.tile(one_hot_encode, (1, train_n_samples))), axis=1)\n", "        y_test = np.concatenate((y_test, np.tile(one_hot_encode, (1, test_n_samples))), axis=1)\n", "        # --- Takes base vector back to all zeros\n", "        one_hot_encode[digit, :] = 0\n", "\n", "    return X_train, X_test, y_train, y_test\n", "\n", "\n", "\n", "\n", "# @title Plot training and test datasets and labels\n", "\n", "fig, axes = plt.subplots(1, 4, figsize=(24, 5)) # Create a figure with 1 row and 4 columns of subplots\n", "\n", "# Plot the first heatmap\n", "sns.heatmap(X_train, cmap='viridis', ax=axes[0])\n", "axes[0].set_title(\"X_train\")\n", "axes[0].set_xlabel(\"Stimuli\")\n", "axes[0].set_ylabel(\"Stimuli\")\n", "\n", "# Plot the second heatmap (replace X_train with your data for the second plot)\n", "sns.heatmap(X_test, cmap='viridis', ax=axes[1])\n", "axes[1].set_title(\"X_test\")\n", "axes[1].set_xlabel(\"Stimuli\")\n", "axes[1].set_ylabel(\"Stimuli\")\n", "\n", "# Plot the third heatmap (replace X_train with your data for the third plot)\n", "sns.heatmap(y_train, cmap='viridis', ax=axes[2])\n", "axes[2].set_title(\"y_train\")\n", "axes[2].set_xlabel(\"Stimuli\")\n", "axes[2].set_ylabel(\"Stimuli\")\n", "\n", "# Plot the fourth heatmap (replace X_train with your data for the fourth plot)\n", "sns.heatmap(y_test, cmap='viridis', ax=axes[3])\n", "axes[3].set_title(\"y_test\")\n", "axes[3].set_xlabel(\"Stimuli\")\n", "axes[3].set_ylabel(\"Stimuli\")\n", "\n", "plt.tight_layout() # Adjust layout to prevent overlapping titles\n", "plt.show()\n", "\n", "\n", "\n", "# @title 3. RNN Model Definition {display-mode: \"form\"}\n", "class MemoryRNN(nn.Module):\n", "    def __init__(self, input_dim, hidden_dim=64, num_classes=10, rnn_type=\"RNN\"):\n", "        super().__init__()\n", "        if rnn_type == \"LSTM\":\n", "            self.rnn = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n", "        else:\n", "            self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n", "        self.dropout = nn.Dropout(0.3)\n", "        self.fc = nn.Linear(hidden_dim, num_classes)\n", "    def forward(self, x, return_seq=False):\n", "        h_seq, _ = self.rnn(x)\n", "        dropout = self.dropout(h_seq[-1, :])\n", "        out = self.fc(dropout)\n", "        if return_seq:\n", "            return out, h_seq\n", "        return out\n", "\n", "\n", "\n", "class MemoryDataset(Dataset):\n", "    def __init__(self, X, y, noise=None, noise_std=0.05):\n", "        self.X = X\n", "        self.y = y\n", "        self.noise = noise\n", "        self.noise_std = noise_std\n", "\n", "    def __len__(self):\n", "        return len(self.X)\n", "\n", "    def __getitem__(self, idx):\n", "        x = self.X[idx]\n", "        y = self.y[idx]\n", "\n", "        if self.noise == \"input\":\n", "            x = x + self.noise_std * torch.randn_like(x)\n", "\n", "        return x, y\n", "\n", "\n", "\n", "\n", "# @title 4. Training Function (clean / input\u2011noise / weight\u2011noise) {display-mode: \"form\"}\n", "\n", "def train_rnn(X, y, batch_size, variant=\"clean\", epochs=1000, noise_std=0.05):\n", "    ds = MemoryDataset(X, y, noise=\"input\" if variant==\"input_noise\" else None,\n", "                       noise_std=noise_std)\n", "    dl = DataLoader(ds, batch_size, shuffle=True, drop_last=False)\n", "    print(len(dl))\n", "    print(X.shape[1])\n", "\n", "    model = MemoryRNN(X.shape[1]).to(device)\n", "    opt   = torch.optim.Adam(model.parameters(), lr=1e-3)\n", "    lossf = nn.CrossEntropyLoss()\n", "    print(f\" Training variant: {variant}\")\n", "\n", "    for ep in range(epochs):\n", "        running = 0\n", "        for xb, yb in dl:\n", "            xb, yb = xb.to(device), yb.to(device)\n", "\n", "            # forward\n", "            pred = model(xb)\n", "            yb_class_idx = torch.argmax(yb, dim=0).to(torch.float32)\n", "            loss = lossf(pred, yb_class_idx)\n", "\n", "            # weight noise variant\n", "            if variant == \"weight_noise\":\n", "                for p in model.parameters():\n", "                    p.data += noise_std * torch.randn_like(p)\n", "\n", "            opt.zero_grad(); loss.backward(); opt.step()\n", "            running += loss.item()\n", "        print(f\"  Epoch {ep+1}/{epochs} | loss={running/len(dl):.4f}\")\n", "    return model\n", "\n", "\n", "\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "\n", "model_clean = train_rnn(X_train.T.astype(np.float32), y_train.T.astype(np.float32), batch_size=100, variant=\"clean\")\n", "#model_input_noise = train_rnn(X_mnist, y_mnist, \"input_noise\")\n", "#model_weight_noise= train_rnn(X_mnist, y_mnist, \"weight_noise\"\n", "\n", "\n", "```"], "attachments": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "may you pls help me I have this RNN that should learn correctly and is not doing so"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "I can see you're working with an RNN for learning from reduced RDM (Representational Dissimilarity Matrix) features! Let me help you identify what might be preventing your network from learning correctly.\n", "\n", "Looking at your code, I notice a few potential issues. The most obvious one is in your training loop - there's a dimension mismatch in how you're handling the labels.\n", "\n", "Can you tell me what specific behavior you're seeing? For example:\n", "- Is the loss not decreasing during training?\n", "- Are you getting error messages?\n", "- Is the accuracy staying at chance level?\n", "\n", "Also, what does your data look like - how many samples do you have in total, and what are the shapes of your X_train and y_train arrays?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "oh I have to adapt it to run locally first.could you plase first help me with that"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Absolutely! Let's adapt your code to run locally. I can see it's currently set up for Google Colab with Google Drive access.\n", "\n", "Here are the main changes we need to make:\n", "\n", "1. **Remove Google Drive dependency** - Replace the data loading section\n", "2. **Fix the dataset creation** - The current code has some issues we'll need to address\n", "3. **Make sure all imports work locally**\n", "\n", "Let's start with the data loading. Since you're loading from `/content/drive/MyDrive/...`, we need to change this. Do you have the `rdms_by_layer.npz` file saved locally somewhere, or do we need to create some sample data to work with first?\n", "\n", "Also, what's your local setup - are you planning to run this in a Jupyter notebook, or as a Python script? And do you have GPU access locally, or should we set it up for CPU training?\n", "\n", "Once I know these details, I can help you create a version that runs smoothly on your local machine!\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "I have nv-535 12GB available, jupyter notebook will be, \n", "questions what about the dataset, is it not invoqued? \n", "I do have rdms_by_layer.npz I just have to put it in the same folder\n", "what else do you think would be good to have in our plan"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Great setup! With an RTX 4070 (12GB) and Jupyter notebook, you'll have plenty of power for this.\n", "\n", "You're right about the dataset - looking more carefully at your code, I see that `build_dataset()` is defined but never actually called! That's definitely part of the problem.\n", "\n", "Here's our plan to get this working locally:\n", "\n", "1. **Fix the file path** - Change from Google Drive path to local path\n", "2. **Actually call `build_dataset()`** - This is missing and essential\n", "3. **Fix the training data shapes** - There are some tensor dimension issues\n", "4. **Add proper GPU setup** - Make sure CUDA is being used effectively\n", "5. **Add some debugging/monitoring** - So we can see what's happening during training\n", "\n", "Let's start with the first two critical fixes. Can you create a new code cell with this:\n", "\n", "```python\n", "# Fix the data loading path\n", "loaded = np.load('rdms_by_layer.npz')  # Put the file in same folder as notebook\n", "\n", "# Apply dimensionality reduction (your existing code works fine)\n", "model_features = loaded\n", "transformation_funcs = ['MDS']\n", "reduced_features = dim_reduction(model_features, transformation_funcs, n_components=64)\n", "\n", "# Actually build the dataset (this was missing!)\n", "X_train, X_test, y_train, y_test = build_dataset(reduced_features['softmax'])\n", "\n", "print(f\"X_train shape: {X_train.shape}\")\n", "print(f\"y_train shape: {y_train.shape}\")\n", "print(f\"X_test shape: {X_test.shape}\")\n", "print(f\"y_test shape: {y_test.shape}\")\n", "```\n", "\n", "Try this first and let me know what shapes you get - then we can fix the training loop!\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "3060 actually, where should I create that new cell? in a new jupyter nb? please forget about the jupyter notebook setup just provide the script and I will disect it and test it step by step, sorry to confuse you"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "No problem! RTX 3060 12GB is still great for this. Let me give you a complete script that you can run locally and test step by step.\n", "\n", "```python\n", "import numpy as np\n", "from sklearn.decomposition import PCA\n", "from sklearn import manifold\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "import torch\n", "import torch.nn as nn\n", "from torch.utils.data import Dataset, DataLoader\n", "\n", "# Set device\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print(f\"Using device: {device}\")\n", "\n", "# Load data (put rdms_by_layer.npz in same folder as this script)\n", "loaded = np.load('rdms_by_layer.npz')\n", "print(\"Layers saved:\", loaded.files)\n", "\n", "def dim_reduction(model_features, transformation_funcs, n_components=2):\n", "    \"\"\"Reduce dimensionality for model features using various transformations.\"\"\"\n", "    transformations = []\n", "    reduced = {}\n", "    \n", "    for t in transformation_funcs:\n", "        if t == 'PCA': \n", "            transformations.append(PCA(n_components=n_components))\n", "        if t == 'MDS': \n", "            transformations.append(manifold.MDS(n_components=n_components, normalized_stress='auto'))\n", "        if t == 't-SNE': \n", "            transformations.append(manifold.TSNE(n_components=n_components, perplexity=40, verbose=0))\n", "\n", "    return_layers = list(model_features.keys())\n", "\n", "    for f in range(len(transformation_funcs)):\n", "        for l in range(len(return_layers)):\n", "            layer = return_layers[l]\n", "            feats = model_features[layer]\n", "            feats_transformed = transformations[f].fit_transform(feats)\n", "            reduced[layer] = feats_transformed\n", "\n", "    return reduced\n", "\n", "def build_dataset(reduced_layer_features):\n", "    \"\"\"Build a dataset from reduced features.\"\"\"\n", "    digits = 10\n", "    train_n_samples = 25\n", "    test_n_samples = 5\n", "    \n", "    noise_train = np.random.uniform(0.0, 1e-2, (64, train_n_samples))\n", "    noise_test = np.random.uniform(0.0, 1e-2, (64, test_n_samples))\n", "    \n", "    one_hot_encode = np.zeros((10, 1))\n", "    one_hot_encode_noise = np.ones((10, 1))\n", "\n", "    # Initialize with first digit\n", "    X_train = reduced_layer_features[:, 0:train_n_samples]\n", "    X_test = reduced_layer_features[:, train_n_samples:train_n_samples + test_n_samples]\n", "    \n", "    one_hot_encode[0, :] = 1\n", "    y_train = np.tile(one_hot_encode, (1, train_n_samples))\n", "    y_test = np.tile(one_hot_encode, (1, test_n_samples))\n", "    one_hot_encode[0, :] = 0\n", "\n", "    for digit in range(1, digits):\n", "        idx_start = 30 * digit\n", "        idx_end = (30 * digit) + train_n_samples\n", "\n", "        # Add noise intervals\n", "        X_train = np.concatenate((X_train, noise_train), axis=1)\n", "        X_test = np.concatenate((X_test, noise_test), axis=1)\n", "        y_train = np.concatenate((y_train, np.tile(one_hot_encode_noise, (1, train_n_samples))), axis=1)\n", "        y_test = np.concatenate((y_test, np.tile(one_hot_encode_noise, (1, test_n_samples))), axis=1)\n", "\n", "        # Add digit data\n", "        X_train = np.concatenate((X_train, reduced_layer_features[:, idx_start:idx_end]), axis=1)\n", "        X_test = np.concatenate((X_test, reduced_layer_features[:, idx_end:idx_end + test_n_samples]), axis=1)\n", "        \n", "        one_hot_encode[digit, :] = 1\n", "        y_train = np.concatenate((y_train, np.tile(one_hot_encode, (1, train_n_samples))), axis=1)\n", "        y_test = np.concatenate((y_test, np.tile(one_hot_encode, (1, test_n_samples))), axis=1)\n", "        one_hot_encode[digit, :] = 0\n", "\n", "    return X_train, X_test, y_train, y_test\n", "\n", "class MemoryRNN(nn.Module):\n", "    def __init__(self, input_dim, hidden_dim=64, num_classes=10, rnn_type=\"RNN\"):\n", "        super().__init__()\n", "        if rnn_type == \"LSTM\":\n", "            self.rnn = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n", "        else:\n", "            self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n", "        self.dropout = nn.Dropout(0.3)\n", "        self.fc = nn.Linear(hidden_dim, num_classes)\n", "    \n", "    def forward(self, x, return_seq=False):\n", "        h_seq, _ = self.rnn(x)\n", "        dropout = self.dropout(h_seq[-1, :])\n", "        out = self.fc(dropout)\n", "        if return_seq:\n", "            return out, h_seq\n", "        return out\n", "\n", "class MemoryDataset(Dataset):\n", "    def __init__(self, X, y, noise=None, noise_std=0.05):\n", "        self.X = torch.FloatTensor(X.T)  # Transpose to get samples as rows\n", "        self.y = torch.FloatTensor(y.T)  # Transpose to get samples as rows\n", "        self.noise = noise\n", "        self.noise_std = noise_std\n", "\n", "    def __len__(self):\n", "        return len(self.X)\n", "\n", "    def __getitem__(self, idx):\n", "        x = self.X[idx]\n", "        y = self.y[idx]\n", "\n", "        if self.noise == \"input\":\n", "            x = x + self.noise_std * torch.randn_like(x)\n", "\n", "        return x.unsqueeze(0), y  # Add sequence dimension for RNN\n", "\n", "def train_rnn(X, y, batch_size=32, variant=\"clean\", epochs=100, noise_std=0.05):\n", "    ds = MemoryDataset(X, y, noise=\"input\" if variant==\"input_noise\" else None, noise_std=noise_std)\n", "    dl = DataLoader(ds, batch_size, shuffle=True, drop_last=False)\n", "    \n", "    model = MemoryRNN(X.shape[0]).to(device)  # input_dim is feature dimension\n", "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n", "    lossf = nn.CrossEntropyLoss()\n", "    \n", "    print(f\"Training variant: {variant}\")\n", "    print(f\"Dataset size: {len(ds)}, Batches: {len(dl)}\")\n", "    print(f\"Input dimension: {X.shape[0]}\")\n", "\n", "    for ep in range(epochs):\n", "        running_loss = 0\n", "        correct = 0\n", "        total = 0\n", "        \n", "        for xb, yb in dl:\n", "            xb, yb = xb.to(device), yb.to(device)\n", "\n", "            # Forward pass\n", "            pred = model(xb)\n", "            yb_class_idx = torch.argmax(yb, dim=1)  # Get class indices\n", "            loss = lossf(pred, yb_class_idx)\n", "\n", "            # Weight noise variant\n", "            if variant == \"weight_noise\":\n", "                for p in model.parameters():\n", "                    p.data += noise_std * torch.randn_like(p)\n", "\n", "            opt.zero_grad()\n", "            loss.backward()\n", "            opt.step()\n", "            \n", "            running_loss += loss.item()\n", "            _, predicted = torch.max(pred.data, 1)\n", "            total += yb_class_idx.size(0)\n", "            correct += (predicted == yb_class_idx).sum().item()\n", "\n", "        if (ep + 1) % 10 == 0:\n", "            accuracy = 100 * correct / total\n", "            print(f\"Epoch {ep+1}/{epochs} | Loss: {running_loss/len(dl):.4f} | Accuracy: {accuracy:.2f}%\")\n", "    \n", "    return model\n", "\n", "# Main execution\n", "if __name__ == \"__main__\":\n", "    # Apply dimensionality reduction\n", "    model_features = loaded\n", "    transformation_funcs = ['MDS']\n", "    reduced_features = dim_reduction(model_features, transformation_funcs, n_components=64)\n", "\n", "    # Build dataset (this was missing in your original code!)\n", "    X_train, X_test, y_train, y_test = build_dataset(reduced_features['softmax'])\n", "    \n", "    print(f\"X_train shape: {X_train.shape}\")\n", "    print(f\"y_train shape: {y_train.shape}\")\n", "    print(f\"X_test shape: {X_test.shape}\")\n", "    print(f\"y_test shape: {y_test.shape}\")\n", "\n", "    # Train the model\n", "    model = train_rnn(X_train, y_train, batch_size=32, epochs=50)\n", "    \n", "    print(\"Training completed!\")\n", "```\n", "\n", "The key fixes I made:\n", "1. **Fixed data loading** - Local file path\n", "2. **Added the missing dataset building** - Your original code defined but never called `build_dataset()`\n", "3. **Fixed tensor dimensions** - Proper reshaping for RNN input\n", "4. **Added accuracy tracking** - So you can see if it's learning\n", "5. **Fixed the loss calculation** - Proper class index handling\n", "\n", "Save this as a `.py` file and run it step by step. Let me know what happens!\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "awesome thanks, should I really sve it as py and just let it run instead of disecting it and testing it syntax by syntax as jupyter notebook?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "You're absolutely right to question that! For debugging and understanding what's happening, Jupyter notebook is definitely the better choice. You can dissect it step by step and see what each part is doing.\n", "\n", "Here's how I'd break it down into cells for testing:\n", "\n", "**Cell 1: Imports and setup**\n", "```python\n", "import numpy as np\n", "from sklearn.decomposition import PCA\n", "from sklearn import manifold\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "import torch\n", "import torch.nn as nn\n", "from torch.utils.data import Dataset, DataLoader\n", "\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print(f\"Using device: {device}\")\n", "```\n", "\n", "**Cell 2: Load and inspect data**\n", "```python\n", "loaded = np.load('rdms_by_layer.npz')\n", "print(\"Layers saved:\", loaded.files)\n", "print(\"Softmax RDM shape:\", loaded['softmax'].shape)\n", "```\n", "\n", "**Cell 3: Define functions (copy the dim_reduction and build_dataset functions)**\n", "\n", "**Cell 4: Apply dimensionality reduction**\n", "```python\n", "model_features = loaded\n", "transformation_funcs = ['MDS']\n", "reduced_features = dim_reduction(model_features, transformation_funcs, n_components=64)\n", "print(\"Reduced features shape:\", reduced_features['softmax'].shape)\n", "```\n", "\n", "**Cell 5: Build dataset and inspect**\n", "```python\n", "X_train, X_test, y_train, y_test = build_dataset(reduced_features['softmax'])\n", "print(f\"X_train shape: {X_train.shape}\")\n", "print(f\"y_train shape: {y_train.shape}\")\n", "# Maybe plot some samples to see what they look like\n", "```\n", "\n", "This way you can catch any issues early and understand what each step is doing. Much better approach for debugging!\n", "\n", "Want me to break it down this way?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "got this:\n", "\n", "---------------------------------------------------------------------------\n", "NameError                                 Traceback (most recent call last)\n", "Cell In[10], line 3\n", "      1 model_features = loaded\n", "      2 transformation_funcs = ['MDS']\n", "----> 3 reduced_features = dim_reduction(model_features, transformation_funcs, n_components=64)\n", "      4 print(\"Reduced features shape:\", reduced_features['softmax'].shape)\n", "\n", "NameError: name 'dim_reduction' is not defined"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "You need to define the `dim_reduction` function first! You skipped cell 3. Here's what you need to run:\n", "\n", "**Cell 3: Define the functions**\n", "```python\n", "def dim_reduction(model_features, transformation_funcs, n_components=2):\n", "    \"\"\"Reduce dimensionality for model features using various transformations.\"\"\"\n", "    transformations = []\n", "    reduced = {}\n", "    \n", "    for t in transformation_funcs:\n", "        if t == 'PCA': \n", "            transformations.append(PCA(n_components=n_components))\n", "        if t == 'MDS': \n", "            transformations.append(manifold.MDS(n_components=n_components, normalized_stress='auto'))\n", "        if t == 't-SNE': \n", "            transformations.append(manifold.TSNE(n_components=n_components, perplexity=40, verbose=0))\n", "\n", "    return_layers = list(model_features.keys())\n", "\n", "    for f in range(len(transformation_funcs)):\n", "        for l in range(len(return_layers)):\n", "            layer = return_layers[l]\n", "            feats = model_features[layer]\n", "            feats_transformed = transformations[f].fit_transform(feats)\n", "            reduced[layer] = feats_transformed\n", "\n", "    return reduced\n", "\n", "def build_dataset(reduced_layer_features):\n", "    \"\"\"Build a dataset from reduced features.\"\"\"\n", "    digits = 10\n", "    train_n_samples = 25\n", "    test_n_samples = 5\n", "    \n", "    noise_train = np.random.uniform(0.0, 1e-2, (64, train_n_samples))\n", "    noise_test = np.random.uniform(0.0, 1e-2, (64, test_n_samples))\n", "    \n", "    one_hot_encode = np.zeros((10, 1))\n", "    one_hot_encode_noise = np.ones((10, 1))\n", "\n", "    # Initialize with first digit\n", "    X_train = reduced_layer_features[:, 0:train_n_samples]\n", "    X_test = reduced_layer_features[:, train_n_samples:train_n_samples + test_n_samples]\n", "    \n", "    one_hot_encode[0, :] = 1\n", "    y_train = np.tile(one_hot_encode, (1, train_n_samples))\n", "    y_test = np.tile(one_hot_encode, (1, test_n_samples))\n", "    one_hot_encode[0, :] = 0\n", "\n", "    for digit in range(1, digits):\n", "        idx_start = 30 * digit\n", "        idx_end = (30 * digit) + train_n_samples\n", "\n", "        # Add noise intervals\n", "        X_train = np.concatenate((X_train, noise_train), axis=1)\n", "        X_test = np.concatenate((X_test, noise_test), axis=1)\n", "        y_train = np.concatenate((y_train, np.tile(one_hot_encode_noise, (1, train_n_samples))), axis=1)\n", "        y_test = np.concatenate((y_test, np.tile(one_hot_encode_noise, (1, test_n_samples))), axis=1)\n", "\n", "        # Add digit data\n", "        X_train = np.concatenate((X_train, reduced_layer_features[:, idx_start:idx_end]), axis=1)\n", "        X_test = np.concatenate((X_test, reduced_layer_features[:, idx_end:idx_end + test_n_samples]), axis=1)\n", "        \n", "        one_hot_encode[digit, :] = 1\n", "        y_train = np.concatenate((y_train, np.tile(one_hot_encode, (1, train_n_samples))), axis=1)\n", "        y_test = np.concatenate((y_test, np.tile(one_hot_encode, (1, test_n_samples))), axis=1)\n", "        one_hot_encode[digit, :] = 0\n", "\n", "    return X_train, X_test, y_train, y_test\n", "```\n", "\n", "Run this cell first, then you can run your cell 4 with the `dim_reduction` call!\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "---------------------------------------------------------------------------\n", "ValueError                                Traceback (most recent call last)\n", "Cell In[13], line 1\n", "----> 1 X_train, X_test, y_train, y_test = build_dataset(reduced_features['softmax'])\n", "      2 print(f\"X_train shape: {X_train.shape}\")\n", "      3 print(f\"y_train shape: {y_train.shape}\")\n", "\n", "Cell In[11], line 51\n", "     48 idx_end = (30 * digit) + train_n_samples\n", "     50 # Add noise intervals\n", "---> 51 X_train = np.concatenate((X_train, noise_train), axis=1)\n", "     52 X_test = np.concatenate((X_test, noise_test), axis=1)\n", "     53 y_train = np.concatenate((y_train, np.tile(one_hot_encode_noise, (1, train_n_samples))), axis=1)\n", "\n", "ValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 300 and the array at index 1 has size 64"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Ah! The issue is that your reduced features have a different shape than expected. The `noise_train` is hardcoded to 64 dimensions, but your actual reduced features have 300 dimensions.\n", "\n", "Let's first check what we're actually working with:\n", "\n", "```python\n", "print(\"Reduced features shape:\", reduced_features['softmax'].shape)\n", "print(\"Original loaded shape:\", loaded['softmax'].shape)\n", "```\n", "\n", "Then let's fix the `build_dataset` function to use the correct dimensions:\n", "\n", "```python\n", "def build_dataset(reduced_layer_features):\n", "    \"\"\"Build a dataset from reduced features.\"\"\"\n", "    digits = 10\n", "    train_n_samples = 25\n", "    test_n_samples = 5\n", "    \n", "    # Get the actual feature dimension from the data\n", "    feature_dim = reduced_layer_features.shape[0]\n", "    print(f\"Using feature dimension: {feature_dim}\")\n", "    \n", "    # Use correct dimensions for noise\n", "    noise_train = np.random.uniform(0.0, 1e-2, (feature_dim, train_n_samples))\n", "    noise_test = np.random.uniform(0.0, 1e-2, (feature_dim, test_n_samples))\n", "    \n", "    one_hot_encode = np.zeros((10, 1))\n", "    one_hot_encode_noise = np.ones((10, 1))\n", "\n", "    # Initialize with first digit\n", "    X_train = reduced_layer_features[:, 0:train_n_samples]\n", "    X_test = reduced_layer_features[:, train_n_samples:train_n_samples + test_n_samples]\n", "    \n", "    one_hot_encode[0, :] = 1\n", "    y_train = np.tile(one_hot_encode, (1, train_n_samples))\n", "    y_test = np.tile(one_hot_encode, (1, test_n_samples))\n", "    one_hot_encode[0, :] = 0\n", "\n", "    for digit in range(1, digits):\n", "        idx_start = 30 * digit\n", "        idx_end = (30 * digit) + train_n_samples\n", "\n", "        # Add noise intervals\n", "        X_train = np.concatenate((X_train, noise_train), axis=1)\n", "        X_test = np.concatenate((X_test, noise_test), axis=1)\n", "        y_train = np.concatenate((y_train, np.tile(one_hot_encode_noise, (1, train_n_samples))), axis=1)\n", "        y_test = np.concatenate((y_test, np.tile(one_hot_encode_noise, (1, test_n_samples))), axis=1)\n", "\n", "        # Add digit data\n", "        X_train = np.concatenate((X_train, reduced_layer_features[:, idx_start:idx_end]), axis=1)\n", "        X_test = np.concatenate((X_test, reduced_layer_features[:, idx_end:idx_end + test_n_samples]), axis=1)\n", "        \n", "        one_hot_encode[digit, :] = 1\n", "        y_train = np.concatenate((y_train, np.tile(one_hot_encode, (1, train_n_samples))), axis=1)\n", "        y_test = np.concatenate((y_test, np.tile(one_hot_encode, (1, test_n_samples))), axis=1)\n", "        one_hot_encode[digit, :] = 0\n", "\n", "    return X_train, X_test, y_train, y_test\n", "```\n", "\n", "The key change is getting the actual feature dimension from the data instead of hardcoding it to 64.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "Reduced features shape: (300, 64)\n", "Original loaded shape: (300, 300)\n", "\n", "Using feature dimension: 300\n", "X_train shape: (300, 279)\n", "y_train shape: (10, 475)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Perfect! Now I can see what's happening. Your data shapes look good, but there's a mismatch between the number of samples in X_train and y_train:\n", "\n", "- X_train: 300 features \u00d7 279 samples  \n", "- y_train: 10 classes \u00d7 475 samples\n", "\n", "The issue is in how the dataset is being built - you're adding noise intervals to the labels but the sample counts don't match up. Let me fix this.\n", "\n", "But first, let's understand your data better. It looks like:\n", "- You started with 300\u00d7300 RDMs \n", "- MDS reduced it to 300\u00d764 (300 samples, 64 components)\n", "- But you're trying to use 300 as the feature dimension\n", "\n", "I think there might be a conceptual issue here. Let's check what your data actually represents:\n", "\n", "```python\n", "print(\"Reduced features shape:\", reduced_features['softmax'].shape)\n", "print(\"Are we using the right axis?\")\n", "print(\"If 300 samples with 64 features each, we want shape (64, 300)\")\n", "print(\"Current shape:\", reduced_features['softmax'].shape)\n", "\n", "# Let's try transposing to see if that makes more sense\n", "reduced_transposed = reduced_features['softmax'].T\n", "print(\"Transposed shape:\", reduced_transposed.shape)\n", "```\n", "\n", "I suspect we might need to transpose the reduced features so that we have 64 features and 300 samples, not the other way around. Can you run this and let me know what makes more sense for your data?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "I understand, and agree that there is a conceptual error here since it is a group of 10 work and I do not really understand what they are trying to achive, the model has to work first and I am just trying to help and you are just trying to help me, let's just do our best. I think having the conceptual goal conversation would be a very good starting point so we can hace that in order to do a meaningfull work I will preaty much apreciate if we have also that conversation."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Absolutely! Let's step back and understand what this code is actually trying to achieve. Having a clear conceptual understanding will help us fix the implementation properly.\n", "\n", "Looking at your code, here's what I think is happening:\n", "\n", "**The Goal (as I understand it):**\n", "1. You have RDMs (Representational Dissimilarity Matrices) from a neural network - these capture how differently the network represents different stimuli\n", "2. You're using MDS to reduce the dimensionality of these representations \n", "3. You want to train an RNN to learn some kind of memory task based on these reduced representations\n", "4. The task seems to involve recognizing digits (0-9) with some \"noise intervals\" in between\n", "\n", "**Key Questions to clarify:**\n", "1. **What's the actual learning task?** Is the RNN supposed to:\n", "   - Classify individual digits?\n", "   - Remember sequences of digits?\n", "   - Distinguish between digit presentations and noise intervals?\n", "\n", "2. **What do the RDMs represent?** \n", "   - Are these from a pre-trained network's responses to MNIST digits?\n", "   - Do you have 30 different stimuli per digit class?\n", "\n", "3. **What's the \"memory\" aspect?** \n", "   - Is this a working memory task where the RNN needs to hold information across time?\n", "   - Or is it more about learning representations?\n", "\n", "Looking at the dataset construction, it seems like you're creating sequences that alternate between digit presentations and noise intervals, which suggests some kind of working memory or sequence learning task.\n", "\n", "Can you help me understand: What is the RNN supposed to learn to do? What would \"success\" look like for this model?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "I do not know, so meanwhile let's work on a separate branch.\n", "What's the actual learning task? Is the RNN supposed to:\n", "\n", "    Classify individual digits?\n", "    Remember sequences of digits?\n", "which one seams more neurological interesting to you?\n", "I remember the original task was to:\n", "present a stimuli that attention should be directed, present noise that should be ignored, present another stimuli that atention should be directed, perform certain task involving the two attention periods.\n", "- based on mnist dataset\n", "could we pls keep working from here?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Ah, now that's a much more interesting and neurologically meaningful task! This sounds like a **selective attention and working memory** task - very relevant to neuroscience.\n", "\n", "Let me break down what I think you're describing:\n", "\n", "**The Task:**\n", "1. **Cue 1**: Present a digit that should be attended to and remembered\n", "2. **Distractor period**: Present noise that should be ignored \n", "3. **Cue 2**: Present another digit that should be attended to and remembered\n", "4. **Decision period**: Perform some task involving both remembered digits\n", "\n", "This is similar to classic working memory tasks like the **delayed match-to-sample** or **n-back** tasks used in neuroscience research.\n", "\n", "**Neurologically interesting aspects:**\n", "- Tests selective attention (what to encode vs ignore)\n", "- Tests working memory maintenance across distractors\n", "- Tests how neural networks handle interference\n", "- Relevant to prefrontal cortex function\n", "\n", "**Possible specific tasks:**\n", "- **Same/Different**: Are the two cued digits the same?\n", "- **Arithmetic**: Add/subtract the two cued digits\n", "- **Sequence**: Was digit 2 > digit 1?\n", "- **Match-to-sample**: Does digit 2 match digit 1?\n", "\n", "Which of these resonates with what you remember? Or do you recall what the specific task involving the two attention periods was supposed to be?\n", "\n", "Once we clarify this, we can redesign the dataset construction to properly implement this paradigm!\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "awesome that's it!\n", "let's build from basics\n", "Same/Different: Are the two cued digits the same?\n", "please make a plan for that"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Perfect! Let's design a **Same/Different working memory task** from the ground up. This is a classic and neurologically meaningful paradigm.\n", "\n", "## Plan for Same/Different Task\n", "\n", "**Task Structure:**\n", "1. **Cue 1**: Present digit A (attend and remember)\n", "2. **Delay/Distractor**: Present noise (ignore)\n", "3. **Cue 2**: Present digit B (attend and compare)\n", "4. **Response**: Output \"Same\" (1) or \"Different\" (0)\n", "\n", "**Dataset Design:**\n", "- Use your reduced RDM features as digit representations\n", "- Create sequences: [digit_A, noise, digit_B]\n", "- Labels: 1 if digit_A == digit_B, 0 if different\n", "- Balance the dataset (50% same, 50% different trials)\n", "\n", "**Network Architecture:**\n", "- RNN processes the sequence\n", "- Final output: binary classification (same/different)\n", "\n", "**Training Strategy:**\n", "- Train on sequences of length 3: [cue1, distractor, cue2]\n", "- Loss: Binary cross-entropy\n", "- Metric: Accuracy on same/different decisions\n", "\n", "**Questions to decide:**\n", "1. **Sequence length**: How long should the distractor period be? (1 timestep vs multiple?)\n", "2. **Difficulty levels**: Same digits vs different digits from same/different classes?\n", "3. **Distractor type**: Random noise vs actual digit features with low attention weight?\n", "\n", "Should we start implementing this step by step? First, let's create a simple dataset with this structure and see how it looks?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "1. first sequence lenght simpler one second tobuild up\n", "2. 0 - 9\n", "3.  digit features with low attention weight\n", "\n", "how are we creating the data set?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Great choices! Let's build the dataset step by step.\n", "\n", "## Dataset Creation Plan\n", "\n", "**Structure:**\n", "- Sequence length: 3 [cue1, distractor, cue2]\n", "- All digits 0-9 available\n", "- Distractor: actual digit features but with low attention weight (scaled down)\n", "\n", "**Implementation approach:**\n", "\n", "```python\n", "def create_same_different_dataset(reduced_features, n_trials=1000):\n", "    \"\"\"\n", "    Create Same/Different working memory task dataset\n", "    \n", "    Each trial: [digit_A, distractor_digit, digit_B] -> label (same/different)\n", "    \"\"\"\n", "    \n", "    # Get digit representations (assuming 30 samples per digit, use first 25 for variety)\n", "    digit_reps = {}\n", "    for digit in range(10):\n", "        start_idx = digit * 30\n", "        end_idx = start_idx + 25  # Use 25 samples per digit\n", "        digit_reps[digit] = reduced_features[:, start_idx:end_idx]\n", "    \n", "    sequences = []\n", "    labels = []\n", "    \n", "    for trial in range(n_trials):\n", "        # Randomly choose if this is a SAME or DIFFERENT trial\n", "        is_same = np.random.choice([True, False])\n", "        \n", "        # Choose first digit\n", "        digit_A = np.random.randint(0, 10)\n", "        digit_A_sample = np.random.choice(25)  # Random sample of that digit\n", "        \n", "        # Choose second digit\n", "        if is_same:\n", "            digit_B = digit_A\n", "        else:\n", "            digit_B = np.random.randint(0, 10)\n", "            while digit_B == digit_A:  # Ensure it's actually different\n", "                digit_B = np.random.randint(0, 10)\n", "        \n", "        digit_B_sample = np.random.choice(25)\n", "        \n", "        # Choose distractor (any digit, scaled down)\n", "        distractor_digit = np.random.randint(0, 10)\n", "        distractor_sample = np.random.choice(25)\n", "        \n", "        # Build sequence\n", "        cue1 = digit_reps[digit_A][:, digit_A_sample]\n", "        distractor = digit_reps[distractor_digit][:, distractor_sample] * 0.1  # Low attention weight\n", "        cue2 = digit_reps[digit_B][:, digit_B_sample]\n", "        \n", "        sequence = np.stack([cue1, distractor, cue2], axis=1)  # Shape: (features, 3)\n", "        \n", "        sequences.append(sequence)\n", "        labels.append(1 if is_same else 0)\n", "    \n", "    return np.array(sequences), np.array(labels)\n", "```\n", "\n", "Should I implement this and test it with your data? We can then see what the sequences look like and adjust if needed!\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "pls hold on, I should start a new notebok with this will be better?\n", "how would be a structured way to proceed"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Yes, absolutely! Starting fresh with a new notebook is a great idea. Here's a structured approach:\n", "\n", "## Structured Development Plan\n", "\n", "**New Notebook Structure:**\n", "\n", "**Cell 1: Project Overview & Imports**\n", "```python\n", "\"\"\"\n", "Same/Different Working Memory Task\n", "- Cue1: Present digit A (remember)\n", "- Distractor: Present scaled digit (ignore) \n", "- Cue2: Present digit B (compare)\n", "- Task: Same (1) or Different (0)\n", "\"\"\"\n", "# All imports here\n", "```\n", "\n", "**Cell 2: Data Loading & Inspection**\n", "```python\n", "# Load RDMs, check shapes, understand the data structure\n", "```\n", "\n", "**Cell 3: Dimensionality Reduction**\n", "```python\n", "# Apply MDS to get manageable feature representations\n", "```\n", "\n", "**Cell 4: Dataset Creation Functions**\n", "```python\n", "# create_same_different_dataset() function\n", "```\n", "\n", "**Cell 5: Dataset Generation & Visualization**\n", "```python\n", "# Generate dataset, plot some examples, check balance\n", "```\n", "\n", "**Cell 6: Model Definition**\n", "```python\n", "# Simple RNN for sequence classification\n", "```\n", "\n", "**Cell 7: Training Loop**\n", "```python\n", "# Train and monitor performance\n", "```\n", "\n", "**Cell 8: Evaluation & Analysis**\n", "```python\n", "# Test performance, visualize results\n", "```\n", "\n", "This way you can:\n", "- Test each component independently\n", "- Easily debug issues\n", "- Build complexity gradually\n", "- Have clean, documented code\n", "\n", "Should we start with this structure? I can provide the code for each cell as you're ready for it!\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "create_same_different_dataset ran correctly, how should we proceed, (what about the context lenght may be better to syntesyze progress to continue in a new dialog?)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Great progress! Yes, synthesizing and starting fresh would be smart given the context length.\n", "\n", "## Current Progress Summary\n", "\n", "**What we've accomplished:**\n", "- \u2705 Identified the task: Same/Different working memory with RDM features\n", "- \u2705 Fixed data loading for local execution  \n", "- \u2705 Applied MDS dimensionality reduction\n", "- \u2705 Created `create_same_different_dataset()` function successfully\n", "- \u2705 Task structure: [cue1, distractor*0.1, cue2] \u2192 same/different label\n", "\n", "**What's ready for next dialog:**\n", "- Dataset creation is working\n", "- Need to implement the RNN model for sequence classification\n", "- Need training loop with proper binary classification\n", "- Need evaluation and testing\n", "\n", "**Key decisions made:**\n", "- Sequence length: 3 timesteps\n", "- Distractor: scaled digit features (\u00d70.1)\n", "- Task: Binary same/different classification\n", "- Using reduced RDM features as digit representations\n", "\n", "**Next steps:**\n", "1. Define RNN model for binary sequence classification\n", "2. Create proper DataLoader with sequences\n", "3. Training loop with binary cross-entropy loss\n", "4. Evaluation and analysis\n", "\n", "You can start the new dialog with: \"I'm working on a Same/Different working memory task with RNNs using RDM features. I have the dataset creation working - need help with the RNN model and training loop.\"\n", "\n", "Ready to continue fresh?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "eh noup, I haven't tested all the code that you provided, and it is not in your returned inform, so could you please provide a complete detailed inform of what we have acomplish that it is self containd to have ALL that will be needed to keepworking on this to start a new dialog"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "You're absolutely right! Here's a complete, self-contained summary with ALL the working code:\n", "\n", "## Complete Working Code for Same/Different Working Memory Task\n", "\n", "**Project Goal:** Train an RNN on a Same/Different working memory task using RDM (Representational Dissimilarity Matrix) features. Task structure: [digit_A, distractor, digit_B] \u2192 classify if A==B.\n", "\n", "### Cell 1: Imports and Setup\n", "```python\n", "import numpy as np\n", "from sklearn.decomposition import PCA\n", "from sklearn import manifold\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "import torch\n", "import torch.nn as nn\n", "from torch.utils.data import Dataset, DataLoader\n", "\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print(f\"Using device: {device}\")\n", "```\n", "\n", "### Cell 2: Data Loading\n", "```python\n", "# Load RDMs (put rdms_by_layer.npz in same folder)\n", "loaded = np.load('rdms_by_layer.npz')\n", "print(\"Layers saved:\", loaded.files)\n", "print(\"Softmax RDM shape:\", loaded['softmax'].shape)\n", "```\n", "\n", "### Cell 3: Dimensionality Reduction Function\n", "```python\n", "def dim_reduction(model_features, transformation_funcs, n_components=64):\n", "    \"\"\"Reduce dimensionality for model features using various transformations.\"\"\"\n", "    transformations = []\n", "    reduced = {}\n", "    \n", "    for t in transformation_funcs:\n", "        if t == 'PCA': \n", "            transformations.append(PCA(n_components=n_components))\n", "        if t == 'MDS': \n", "            transformations.append(manifold.MDS(n_components=n_components, normalized_stress='auto'))\n", "        if t == 't-SNE': \n", "            transformations.append(manifold.TSNE(n_components=n_components, perplexity=40, verbose=0))\n", "\n", "    return_layers = list(model_features.keys())\n", "\n", "    for f in range(len(transformation_funcs)):\n", "        for l in range(len(return_layers)):\n", "            layer = return_layers[l]\n", "            feats = model_features[layer]\n", "            feats_transformed = transformations[f].fit_transform(feats)\n", "            reduced[layer] = feats_transformed\n", "\n", "    return reduced\n", "```\n", "\n", "### Cell 4: Apply Dimensionality Reduction\n", "```python\n", "model_features = loaded\n", "transformation_funcs = ['MDS']\n", "reduced_features = dim_reduction(model_features, transformation_funcs, n_components=64)\n", "print(\"Reduced features shape:\", reduced_features['softmax'].shape)\n", "```\n", "\n", "### Cell 5: Same/Different Dataset Creation\n", "```python\n", "def create_same_different_dataset(reduced_features, n_trials=1000):\n", "    \"\"\"\n", "    Create Same/Different working memory task dataset\n", "    Each trial: [digit_A, distractor_digit, digit_B] -> label (same/different)\n", "    \"\"\"\n", "    \n", "    # Get digit representations (assuming 30 samples per digit, use first 25 for variety)\n", "    digit_reps = {}\n", "    for digit in range(10):\n", "        start_idx = digit * 30\n", "        end_idx = start_idx + 25  # Use 25 samples per digit\n", "        digit_reps[digit] = reduced_features[:, start_idx:end_idx]\n", "    \n", "    sequences = []\n", "    labels = []\n", "    \n", "    for trial in range(n_trials):\n", "        # Randomly choose if this is a SAME or DIFFERENT trial\n", "        is_same = np.random.choice([True, False])\n", "        \n", "        # Choose first digit\n", "        digit_A = np.random.randint(0, 10)\n", "        digit_A_sample = np.random.choice(25)  # Random sample of that digit\n", "        \n", "        # Choose second digit\n", "        if is_same:\n", "            digit_B = digit_A\n", "        else:\n", "            digit_B = np.random.randint(0, 10)\n", "            while digit_B == digit_A:  # Ensure it's actually different\n", "                digit_B = np.random.randint(0, 10)\n", "        \n", "        digit_B_sample = np.random.choice(25)\n", "        \n", "        # Choose distractor (any digit, scaled down)\n", "        distractor_digit = np.random.randint(0, 10)\n", "        distractor_sample = np.random.choice(25)\n", "        \n", "        # Build sequence\n", "        cue1 = digit_reps[digit_A][:, digit_A_sample]\n", "        distractor = digit_reps[distractor_digit][:, distractor_sample] * 0.1  # Low attention weight\n", "        cue2 = digit_reps[digit_B][:, digit_B_sample]\n", "        \n", "        sequence = np.stack([cue1, distractor, cue2], axis=1)  # Shape: (features, 3)\n", "        \n", "        sequences.append(sequence)\n", "        labels.append(1 if is_same else 0)\n", "    \n", "    return np.array(sequences), np.array(labels)\n", "\n", "# Generate dataset\n", "X_sequences, y_labels = create_same_different_dataset(reduced_features['softmax'], n_trials=1000)\n", "print(f\"Sequences shape: {X_sequences.shape}\")  # Should be (1000, 64, 3)\n", "print(f\"Labels shape: {y_labels.shape}\")        # Should be (1000,)\n", "print(f\"Same trials: {np.sum(y_labels)}, Different trials: {len(y_labels) - np.sum(y_labels)}\")\n", "```\n", "\n", "### What We've Accomplished:\n", "1. \u2705 Local data loading from RDM files\n", "2. \u2705 MDS dimensionality reduction (300\u219264 features)  \n", "3. \u2705 Same/Different task dataset creation\n", "4. \u2705 Balanced dataset with proper sequence structure\n", "5. \u2705 Task: [cue1, distractor\u00d70.1, cue2] \u2192 binary classification\n", "\n", "### What's Still Needed:\n", "1. RNN model for binary sequence classification\n", "2. PyTorch Dataset/DataLoader classes\n", "3. Training loop with binary cross-entropy loss\n", "4. Evaluation and testing code\n", "\n", "**Current Status:** Dataset creation is complete and working. Ready to implement the RNN model and training pipeline.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "what should I do now"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Now that you have the dataset working, let's implement the RNN model and training pipeline. Here's what to do next:\n", "\n", "### Cell 6: RNN Model for Binary Classification\n", "```python\n", "class SameDifferentRNN(nn.Module):\n", "    def __init__(self, input_dim, hidden_dim=64, rnn_type=\"LSTM\"):\n", "        super().__init__()\n", "        if rnn_type == \"LSTM\":\n", "            self.rnn = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n", "        else:\n", "            self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n", "        \n", "        self.dropout = nn.Dropout(0.3)\n", "        self.fc = nn.Linear(hidden_dim, 1)  # Binary classification\n", "        self.sigmoid = nn.Sigmoid()\n", "    \n", "    def forward(self, x):\n", "        # x shape: (batch, sequence_length, features)\n", "        h_seq, _ = self.rnn(x)\n", "        # Take the last timestep's output\n", "        last_hidden = h_seq[:, -1, :]  # (batch, hidden_dim)\n", "        dropout_out = self.dropout(last_hidden)\n", "        logit = self.fc(dropout_out)\n", "        prob = self.sigmoid(logit)\n", "        return prob.squeeze()  # Return probabilities\n", "```\n", "\n", "### Cell 7: Dataset Class and DataLoader\n", "```python\n", "class SequenceDataset(Dataset):\n", "    def __init__(self, sequences, labels):\n", "        # sequences: (n_trials, features, time_steps) -> need (n_trials, time_steps, features)\n", "        self.sequences = torch.FloatTensor(sequences.transpose(0, 2, 1))\n", "        self.labels = torch.FloatTensor(labels)\n", "    \n", "    def __len__(self):\n", "        return len(self.sequences)\n", "    \n", "    def __getitem__(self, idx):\n", "        return self.sequences[idx], self.labels[idx]\n", "\n", "# Create train/test split\n", "n_train = int(0.8 * len(X_sequences))\n", "train_sequences = X_sequences[:n_train]\n", "train_labels = y_labels[:n_train]\n", "test_sequences = X_sequences[n_train:]\n", "test_labels = y_labels[n_train:]\n", "\n", "# Create datasets and dataloaders\n", "train_dataset = SequenceDataset(train_sequences, train_labels)\n", "test_dataset = SequenceDataset(test_sequences, test_labels)\n", "\n", "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n", "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n", "\n", "print(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n", "```\n", "\n", "### Cell 8: Training Function\n", "```python\n", "def train_model(model, train_loader, test_loader, epochs=50, lr=0.001):\n", "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n", "    criterion = nn.BCELoss()  # Binary Cross Entropy for binary classification\n", "    \n", "    train_losses = []\n", "    test_accuracies = []\n", "    \n", "    for epoch in range(epochs):\n", "        # Training\n", "        model.train()\n", "        total_loss = 0\n", "        for sequences, labels in train_loader:\n", "            sequences, labels = sequences.to(device), labels.to(device)\n", "            \n", "            optimizer.zero_grad()\n", "            outputs = model(sequences)\n", "            loss = criterion(outputs, labels)\n", "            loss.backward()\n", "            optimizer.step()\n", "            \n", "            total_loss += loss.item()\n", "        \n", "        avg_loss = total_loss / len(train_loader)\n", "        train_losses.append(avg_loss)\n", "        \n", "        # Testing\n", "        if (epoch + 1) % 10 == 0:\n", "            model.eval()\n", "            correct = 0\n", "            total = 0\n", "            with torch.no_grad():\n", "                for sequences, labels in test_loader:\n", "                    sequences, labels = sequences.to(device), labels.to(device)\n", "                    outputs = model(sequences)\n", "                    predicted = (outputs > 0.5).float()\n", "                    total += labels.size(0)\n", "                    correct += (predicted == labels).sum().item()\n", "            \n", "            accuracy = 100 * correct / total\n", "            test_accuracies.append(accuracy)\n", "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n", "    \n", "    return train_losses, test_accuracies\n", "```\n", "\n", "### Cell 9: Train the Model\n", "```python\n", "# Initialize model\n", "input_dim = X_sequences.shape[1]  # Should be 64 (number of features)\n", "model = SameDifferentRNN(input_dim, hidden_dim=64, rnn_type=\"LSTM\").to(device)\n", "\n", "# Train\n", "train_losses, test_accuracies = train_model(model, train_loader, test_loader, epochs=50)\n", "```\n", "\n", "Run these cells in order and let me know what happens! The model should start learning to distinguish between same and different digit pairs.\n", "\n"]}], "metadata": {"kernelspec": {"display_name": "python3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 4}