{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def visualize_memory_samples(dataset, num_samples=5, figsize=(16, 3)):\n",
        "    \"\"\"\n",
        "    Visualize memory task samples with detailed annotations\n",
        "\n",
        "    Args:\n",
        "        dataset: Your visual memory dataset\n",
        "        num_samples: Number of samples to visualize\n",
        "        figsize: Figure size per sample\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(num_samples, 6, figsize=(figsize[0], figsize[1] * num_samples))\n",
        "    if num_samples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    fig.suptitle('Visual Memory Task Samples', fontsize=16, y=0.98)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # Get sample with metadata\n",
        "        sequence, label, metadata = dataset.get_sample_with_metadata(i)\n",
        "        seq_len = sequence.shape[0]\n",
        "\n",
        "        target_digit = metadata['target_digit']\n",
        "        probe_digit = metadata['probe_digit']\n",
        "        is_match = metadata['is_match']\n",
        "\n",
        "        print(f\"\\nüìã Sample {i+1}:\")\n",
        "        print(f\"  üéØ Target digit: {target_digit}\")\n",
        "        print(f\"  üîç Probe digit: {probe_digit}\")\n",
        "        print(f\"  ‚úÖ Result: {'MATCH' if is_match else 'NO MATCH'}\")\n",
        "        print(f\"  üè∑Ô∏è Label: {label}\")\n",
        "        print(f\"  üìè Sequence length: {seq_len}\")\n",
        "\n",
        "        # Display sequence\n",
        "        for j in range(min(seq_len, 6)):  # Show up to 6 images\n",
        "            ax = axes[i, j]\n",
        "\n",
        "            # Handle different tensor formats\n",
        "            img = sequence[j]\n",
        "            if len(img.shape) == 3:\n",
        "                img = img.squeeze()\n",
        "            img = img.numpy()\n",
        "\n",
        "            ax.imshow(img, cmap='gray', vmin=0, vmax=1)\n",
        "            ax.axis('off')\n",
        "\n",
        "            if j == 0:\n",
        "                # Target image\n",
        "                ax.set_title(f'TARGET\\n(digit: {target_digit})',\n",
        "                           fontweight='bold', color='blue', fontsize=10)\n",
        "                ax.add_patch(plt.Rectangle((0, 0), img.shape[1]-1, img.shape[0]-1,\n",
        "                                         fill=False, edgecolor='blue', linewidth=2))\n",
        "            elif j == seq_len - 1:\n",
        "                # Probe image\n",
        "                match_text = 'MATCH' if is_match else 'NO MATCH'\n",
        "                color = 'green' if is_match else 'red'\n",
        "                ax.set_title(f'PROBE\\n(digit: {probe_digit})\\n{match_text}',\n",
        "                           fontweight='bold', color=color, fontsize=9)\n",
        "                ax.add_patch(plt.Rectangle((0, 0), img.shape[1]-1, img.shape[0]-1,\n",
        "                                         fill=False, edgecolor=color, linewidth=2))\n",
        "            else:\n",
        "                # Distractor/noise image\n",
        "                ax.set_title(f'NOISE {j}', color='gray', fontsize=9)\n",
        "\n",
        "        # Hide unused subplots\n",
        "        for j in range(seq_len, 6):\n",
        "            axes[i, j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "EPW_pOsvtFNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from collections import Counter, defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class OptimizedNoiseGenerator:\n",
        "    \"\"\"Optimized noise generator with batch processing capabilities\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Pre-compute some values for efficiency\n",
        "        self.patch_sizes = [2, 3, 4]\n",
        "\n",
        "    def gaussian_noise(self, shape, noise_level=0.5):\n",
        "        \"\"\"Generate Gaussian noise image - vectorized\"\"\"\n",
        "        noise = torch.randn(shape) * noise_level\n",
        "        return torch.clamp(noise, 0, 1)\n",
        "\n",
        "    def salt_pepper_noise(self, shape, noise_density=0.3):\n",
        "        \"\"\"Generate salt and pepper noise - optimized\"\"\"\n",
        "        noise = torch.rand(shape)\n",
        "        result = torch.rand(shape) * 0.5 + 0.25  # Base gray level\n",
        "\n",
        "        # Vectorized salt and pepper assignment\n",
        "        salt_mask = noise < noise_density/2\n",
        "        pepper_mask = noise > 1 - noise_density/2\n",
        "\n",
        "        result[salt_mask] = 1.0\n",
        "        result[pepper_mask] = 0.0\n",
        "\n",
        "        return result\n",
        "\n",
        "    def random_patches(self, shape, num_patches=10, patch_size=5):\n",
        "        \"\"\"Generate random square patches - optimized\"\"\"\n",
        "        noise = torch.rand(shape) * 0.3 + 0.35\n",
        "        h, w = shape[-2:]\n",
        "\n",
        "        # Pre-compute valid positions\n",
        "        max_x = max(0, w - patch_size)\n",
        "        max_y = max(0, h - patch_size)\n",
        "\n",
        "        if max_x > 0 and max_y > 0:\n",
        "            for _ in range(num_patches):\n",
        "                x = random.randint(0, max_x)\n",
        "                y = random.randint(0, max_y)\n",
        "                intensity = random.random()\n",
        "                noise[..., y:y+patch_size, x:x+patch_size] = intensity\n",
        "\n",
        "        return noise\n",
        "\n",
        "    def scrambled_mnist_batch(self, images, scramble_intensity=0.8):\n",
        "        \"\"\"Create scrambled versions from a batch of images\"\"\"\n",
        "        results = []\n",
        "        for image in images:\n",
        "            if random.random() > scramble_intensity:\n",
        "                results.append(image)\n",
        "                continue\n",
        "\n",
        "            img_np = image.squeeze().numpy()\n",
        "            h, w = img_np.shape\n",
        "\n",
        "            block_size = random.choice(self.patch_sizes)\n",
        "            for i in range(0, h - block_size, block_size):\n",
        "                for j in range(0, w - block_size, block_size):\n",
        "                    block = img_np[i:i+block_size, j:j+block_size].flatten()\n",
        "                    np.random.shuffle(block)\n",
        "                    img_np[i:i+block_size, j:j+block_size] = block.reshape(block_size, block_size)\n",
        "\n",
        "            results.append(torch.tensor(img_np).unsqueeze(0).float())\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "class OptimizedVisualMemoryDataset(Dataset):\n",
        "    \"\"\"Heavily optimized version of Visual Memory Dataset\"\"\"\n",
        "\n",
        "    def __init__(self, mnist_dataset, num_samples=1000, num_distractors=3,\n",
        "                 noise_types=['gaussian', 'salt_pepper', 'patches', 'scrambled'],\n",
        "                 match_probability=0.5, batch_size=100):\n",
        "\n",
        "        self.mnist_dataset = mnist_dataset\n",
        "        self.num_samples = num_samples\n",
        "        self.num_distractors = num_distractors\n",
        "        self.noise_types = noise_types\n",
        "        self.match_probability = match_probability\n",
        "        self.noise_generator = OptimizedNoiseGenerator()\n",
        "\n",
        "        print(f\"üöÄ Generating {num_samples} optimized visual memory samples...\")\n",
        "\n",
        "        # Step 1: Pre-index MNIST data by digit for faster access\n",
        "        print(\"üìã Pre-indexing MNIST data by digit...\")\n",
        "        self._preindex_mnist_data()\n",
        "\n",
        "        # Step 2: Pre-sample all required indices\n",
        "        print(\"üéØ Pre-sampling required indices...\")\n",
        "        self._presample_indices()\n",
        "\n",
        "        # Step 3: Batch generate samples\n",
        "        print(\"‚ö° Batch generating samples...\")\n",
        "        self.samples = []\n",
        "        self.labels = []\n",
        "        self.metadata = []\n",
        "\n",
        "        self._batch_generate_samples(batch_size)\n",
        "\n",
        "        # Print statistics\n",
        "        self._print_statistics()\n",
        "\n",
        "    def _preindex_mnist_data(self):\n",
        "        \"\"\"Pre-index MNIST data by digit for O(1) access\"\"\"\n",
        "        self.digit_indices = defaultdict(list)\n",
        "\n",
        "        # Build index once\n",
        "        for idx, (_, digit) in enumerate(self.mnist_dataset):\n",
        "            self.digit_indices[digit].append(idx)\n",
        "\n",
        "        # Convert to lists for faster random access\n",
        "        for digit in self.digit_indices:\n",
        "            self.digit_indices[digit] = list(self.digit_indices[digit])\n",
        "\n",
        "        print(f\"   Indexed {len(self.digit_indices)} digit classes\")\n",
        "        for digit, indices in self.digit_indices.items():\n",
        "            print(f\"   Digit {digit}: {len(indices)} samples\")\n",
        "\n",
        "    def _presample_indices(self):\n",
        "        \"\"\"Pre-sample all indices needed for the entire dataset\"\"\"\n",
        "        # Determine matches vs non-matches\n",
        "        num_matches = int(self.num_samples * self.match_probability)\n",
        "        self.match_labels = [1] * num_matches + [0] * (self.num_samples - num_matches)\n",
        "        random.shuffle(self.match_labels)\n",
        "\n",
        "        # Pre-sample target indices\n",
        "        all_indices = list(range(len(self.mnist_dataset)))\n",
        "        self.target_indices = random.choices(all_indices, k=self.num_samples)\n",
        "\n",
        "        # Pre-sample probe indices based on match/no-match\n",
        "        self.probe_indices = []\n",
        "        for i, is_match in enumerate(self.match_labels):\n",
        "            target_idx = self.target_indices[i]\n",
        "            _, target_digit = self.mnist_dataset[target_idx]\n",
        "\n",
        "            if is_match:\n",
        "                # Find same digit, different image\n",
        "                same_digit_candidates = [idx for idx in self.digit_indices[target_digit]\n",
        "                                       if idx != target_idx]\n",
        "                if same_digit_candidates:\n",
        "                    probe_idx = random.choice(same_digit_candidates)\n",
        "                else:\n",
        "                    probe_idx = target_idx  # Fallback to same image\n",
        "            else:\n",
        "                # Find different digit\n",
        "                different_digits = [d for d in self.digit_indices.keys() if d != target_digit]\n",
        "                if different_digits:\n",
        "                    different_digit = random.choice(different_digits)\n",
        "                    probe_idx = random.choice(self.digit_indices[different_digit])\n",
        "                else:\n",
        "                    # Fallback - find any different image\n",
        "                    probe_idx = random.choice([idx for idx in all_indices if idx != target_idx])\n",
        "\n",
        "            self.probe_indices.append(probe_idx)\n",
        "\n",
        "    def _batch_generate_samples(self, batch_size):\n",
        "        \"\"\"Generate samples in batches for efficiency\"\"\"\n",
        "\n",
        "        for batch_start in tqdm(range(0, self.num_samples, batch_size),\n",
        "                               desc=\"Generating batches\"):\n",
        "            batch_end = min(batch_start + batch_size, self.num_samples)\n",
        "            batch_samples, batch_labels, batch_metadata = self._create_batch(\n",
        "                batch_start, batch_end)\n",
        "\n",
        "            self.samples.extend(batch_samples)\n",
        "            self.labels.extend(batch_labels)\n",
        "            self.metadata.extend(batch_metadata)\n",
        "\n",
        "    def _create_batch(self, start_idx, end_idx):\n",
        "        \"\"\"Create a batch of samples efficiently\"\"\"\n",
        "        batch_samples = []\n",
        "        batch_labels = []\n",
        "        batch_metadata = []\n",
        "\n",
        "        # Pre-load all needed MNIST images for this batch\n",
        "        target_indices_batch = self.target_indices[start_idx:end_idx]\n",
        "        probe_indices_batch = self.probe_indices[start_idx:end_idx]\n",
        "\n",
        "        # Load target images\n",
        "        target_images = []\n",
        "        target_digits = []\n",
        "        for idx in target_indices_batch:\n",
        "            img, digit = self.mnist_dataset[idx]\n",
        "            target_images.append(img)\n",
        "            target_digits.append(digit)\n",
        "\n",
        "        # Load probe images\n",
        "        probe_images = []\n",
        "        probe_digits = []\n",
        "        for idx in probe_indices_batch:\n",
        "            img, digit = self.mnist_dataset[idx]\n",
        "            probe_images.append(img)\n",
        "            probe_digits.append(digit)\n",
        "\n",
        "        # Generate noise images in batch\n",
        "        batch_size = end_idx - start_idx\n",
        "        noise_images_batch = self._generate_noise_batch(\n",
        "            target_images[0].shape, batch_size * self.num_distractors)\n",
        "\n",
        "        # Assemble sequences\n",
        "        for i in range(batch_size):\n",
        "            # Get images for this sample\n",
        "            target_img = target_images[i]\n",
        "            probe_img = probe_images[i]\n",
        "\n",
        "            # Get noise images for this sample\n",
        "            noise_start = i * self.num_distractors\n",
        "            noise_end = noise_start + self.num_distractors\n",
        "            noise_imgs = noise_images_batch[noise_start:noise_end]\n",
        "\n",
        "            # Create sequence: [target] + [noise images] + [probe]\n",
        "            sequence = [target_img] + noise_imgs + [probe_img]\n",
        "            sequence_tensor = torch.stack(sequence)\n",
        "\n",
        "            # Create metadata\n",
        "            global_idx = start_idx + i\n",
        "            metadata = {\n",
        "                'target_digit': target_digits[i],\n",
        "                'probe_digit': probe_digits[i],\n",
        "                'is_match': bool(self.match_labels[global_idx]),\n",
        "                'target_idx': target_indices_batch[i],\n",
        "                'probe_idx': probe_indices_batch[i],\n",
        "                'sequence_length': len(sequence)\n",
        "            }\n",
        "\n",
        "            batch_samples.append(sequence_tensor)\n",
        "            batch_labels.append(self.match_labels[global_idx])\n",
        "            batch_metadata.append(metadata)\n",
        "\n",
        "        return batch_samples, batch_labels, batch_metadata\n",
        "\n",
        "    def _generate_noise_batch(self, shape, total_noise_images):\n",
        "        \"\"\"Generate a batch of noise images efficiently\"\"\"\n",
        "        noise_images = []\n",
        "\n",
        "        # Group by noise type for batch processing\n",
        "        noise_counts = {noise_type: 0 for noise_type in self.noise_types}\n",
        "        noise_assignments = []\n",
        "\n",
        "        for _ in range(total_noise_images):\n",
        "            noise_type = random.choice(self.noise_types)\n",
        "            noise_assignments.append(noise_type)\n",
        "            noise_counts[noise_type] += 1\n",
        "\n",
        "        # Generate each type in batch\n",
        "        noise_by_type = {}\n",
        "        for noise_type, count in noise_counts.items():\n",
        "            if count > 0:\n",
        "                if noise_type == 'scrambled':\n",
        "                    # For scrambled, we need actual MNIST images\n",
        "                    random_indices = random.choices(range(len(self.mnist_dataset)), k=count)\n",
        "                    random_images = [self.mnist_dataset[idx][0] for idx in random_indices]\n",
        "                    noise_by_type[noise_type] = self.noise_generator.scrambled_mnist_batch(\n",
        "                        random_images)\n",
        "                else:\n",
        "                    # Generate other noise types in batch\n",
        "                    batch_shape = (count,) + shape\n",
        "                    if noise_type == 'gaussian':\n",
        "                        batch_noise = self.noise_generator.gaussian_noise(batch_shape, 0.6)\n",
        "                    elif noise_type == 'salt_pepper':\n",
        "                        batch_noise = self.noise_generator.salt_pepper_noise(batch_shape, 0.4)\n",
        "                    elif noise_type == 'patches':\n",
        "                        batch_noise = torch.stack([\n",
        "                            self.noise_generator.random_patches(shape, 15, 4)\n",
        "                            for _ in range(count)\n",
        "                        ])\n",
        "\n",
        "                    noise_by_type[noise_type] = [batch_noise[i] for i in range(count)]\n",
        "\n",
        "        # Reconstruct in original order\n",
        "        type_counters = {noise_type: 0 for noise_type in self.noise_types}\n",
        "        for noise_type in noise_assignments:\n",
        "            noise_images.append(noise_by_type[noise_type][type_counters[noise_type]])\n",
        "            type_counters[noise_type] += 1\n",
        "\n",
        "        return noise_images\n",
        "\n",
        "    def _print_statistics(self):\n",
        "        \"\"\"Print dataset statistics\"\"\"\n",
        "        match_count = sum(self.labels)\n",
        "        print(f\"\\n‚úÖ Dataset created:\")\n",
        "        print(f\"  Total samples: {self.num_samples}\")\n",
        "        print(f\"  Match trials: {match_count} ({match_count/self.num_samples*100:.1f}%)\")\n",
        "        print(f\"  No-match trials: {self.num_samples-match_count} ({(self.num_samples-match_count)/self.num_samples*100:.1f}%)\")\n",
        "        print(f\"  Sequence length: {self.num_distractors + 2} (1 target + {self.num_distractors} distractors + 1 probe)\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx], self.labels[idx]\n",
        "\n",
        "    def get_sample_with_metadata(self, idx):\n",
        "        return self.samples[idx], self.labels[idx], self.metadata[idx]\n",
        "\n",
        "    def get_statistics(self):\n",
        "        \"\"\"Get detailed dataset statistics\"\"\"\n",
        "        match_count = sum(self.labels)\n",
        "        total = len(self.labels)\n",
        "\n",
        "        target_digits = [meta['target_digit'] for meta in self.metadata]\n",
        "        probe_digits = [meta['probe_digit'] for meta in self.metadata]\n",
        "\n",
        "        target_counter = Counter(target_digits)\n",
        "        probe_counter = Counter(probe_digits)\n",
        "\n",
        "        return {\n",
        "            'total_samples': total,\n",
        "            'match_trials': match_count,\n",
        "            'nomatch_trials': total - match_count,\n",
        "            'match_percentage': match_count / total * 100,\n",
        "            'target_digit_distribution': dict(target_counter),\n",
        "            'probe_digit_distribution': dict(probe_counter),\n",
        "            'sequence_length': self.num_distractors + 2\n",
        "        }\n",
        "\n",
        "\n",
        "def create_optimized_datasets_for_training(train_size=3000, test_size=1000,\n",
        "                                         num_distractors=3, batch_size=100):\n",
        "    \"\"\"Create optimized datasets for training - much faster\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"CREATING OPTIMIZED TRAINING DATASETS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Import here to avoid issues if not available\n",
        "    from torchvision import datasets, transforms\n",
        "\n",
        "    # Define transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    # Load MNIST datasets\n",
        "    print(\"üìÅ Loading MNIST datasets...\")\n",
        "    mnist_train = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "    mnist_test = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Training dataset\n",
        "    print(\"\\nüöÇ Creating optimized training dataset...\")\n",
        "    train_memory_dataset = OptimizedVisualMemoryDataset(\n",
        "        mnist_train,\n",
        "        num_samples=train_size,\n",
        "        num_distractors=num_distractors,\n",
        "        noise_types=['gaussian', 'salt_pepper', 'patches', 'scrambled'],\n",
        "        match_probability=0.5,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Testing dataset\n",
        "    test_memory_dataset = OptimizedVisualMemoryDataset(\n",
        "        mnist_test,\n",
        "        num_samples=test_size,\n",
        "        num_distractors=num_distractors,\n",
        "        noise_types=['gaussian', 'salt_pepper', 'patches', 'scrambled'],\n",
        "        match_probability=0.5,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_memory_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_memory_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "    return train_memory_dataset, test_memory_dataset, train_loader, test_loader\n",
        "\n",
        "\n",
        "# Compatibility function with original interface\n",
        "def create_datasets_for_training(train_size=3000, test_size=1000, num_distractors=3):\n",
        "    \"\"\"Drop-in replacement for original function with massive speedup\"\"\"\n",
        "    return create_optimized_datasets_for_training(\n",
        "        train_size=train_size,\n",
        "        test_size=test_size,\n",
        "        num_distractors=num_distractors,\n",
        "        batch_size=min(100, train_size // 10)  # Adaptive batch size\n",
        "    )\n",
        "\n",
        "\n",
        "# Example usage\n"
      ],
      "metadata": {
        "id": "mQh1AvpfNCv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset, train_loader, test_loader = create_datasets_for_training(\n",
        "    train_size=1000,\n",
        "    test_size=300,\n",
        "    num_distractors=2\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIUpuh_qstFB",
        "outputId": "6fcbb4c5-06f2-450d-f022-19d4a91abdd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CREATING OPTIMIZED TRAINING DATASETS\n",
            "============================================================\n",
            "üìÅ Loading MNIST datasets...\n",
            "\n",
            "üöÇ Creating optimized training dataset...\n",
            "üöÄ Generating 1000 optimized visual memory samples...\n",
            "üìã Pre-indexing MNIST data by digit...\n",
            "   Indexed 10 digit classes\n",
            "   Digit 5: 5421 samples\n",
            "   Digit 0: 5923 samples\n",
            "   Digit 4: 5842 samples\n",
            "   Digit 1: 6742 samples\n",
            "   Digit 9: 5949 samples\n",
            "   Digit 2: 5958 samples\n",
            "   Digit 3: 6131 samples\n",
            "   Digit 6: 5918 samples\n",
            "   Digit 7: 6265 samples\n",
            "   Digit 8: 5851 samples\n",
            "üéØ Pre-sampling required indices...\n",
            "‚ö° Batch generating samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 10.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Dataset created:\n",
            "  Total samples: 1000\n",
            "  Match trials: 500 (50.0%)\n",
            "  No-match trials: 500 (50.0%)\n",
            "  Sequence length: 4 (1 target + 2 distractors + 1 probe)\n",
            "\n",
            "üß™ Creating optimized testing dataset...\n",
            "üöÄ Generating 300 optimized visual memory samples...\n",
            "üìã Pre-indexing MNIST data by digit...\n",
            "   Indexed 10 digit classes\n",
            "   Digit 7: 1028 samples\n",
            "   Digit 2: 1032 samples\n",
            "   Digit 1: 1135 samples\n",
            "   Digit 0: 980 samples\n",
            "   Digit 4: 982 samples\n",
            "   Digit 9: 1009 samples\n",
            "   Digit 5: 892 samples\n",
            "   Digit 6: 958 samples\n",
            "   Digit 3: 1010 samples\n",
            "   Digit 8: 974 samples\n",
            "üéØ Pre-sampling required indices...\n",
            "‚ö° Batch generating samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 11.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Dataset created:\n",
            "  Total samples: 300\n",
            "  Match trials: 150 (50.0%)\n",
            "  No-match trials: 150 (50.0%)\n",
            "  Sequence length: 4 (1 target + 2 distractors + 1 probe)\n",
            "\n",
            "üéâ Optimized datasets ready for training!\n",
            "Training samples: 1000\n",
            "Testing samples: 300\n",
            "Sequence length: 4\n",
            "Expected speedup: 10-50x faster than original\n",
            "\n",
            "üöÄ Dataset creation completed successfully!\n",
            "Ready for training!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_memory_samples(train_dataset, num_samples=3)"
      ],
      "metadata": {
        "id": "8IwUDHFktuYP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}