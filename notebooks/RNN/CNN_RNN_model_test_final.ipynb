{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["v20zwXDNb-8H","p7gfJR9-iIsw","BNJMXRCiH2Ep"],"authorship_tag":"ABX9TyPdOpCi0yGKRjMU/G5edMys"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"pe85TzhLQztl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd drive/MyDrive/Neuromatch_project/"],"metadata":{"id":"Z9IlQisF1tjh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f_CREs1KPqmI"},"outputs":[],"source":["!pip install torch torchvision matplotlib seaborn tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix, classification_report\n","from tqdm import tqdm\n","import random\n","import pickle\n","import os\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"üì± Using device: {device}\")\n","\n","# Set random seeds for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","random.seed(42)\n","\n","plt.style.use('default')\n","plt.rcParams['figure.figsize'] = (12, 8)\n","\n","print(\"‚úÖ Setup complete!\")"]},{"cell_type":"code","source":["class SimpleCNN(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(SimpleCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.dropout1 = nn.Dropout(0.25)\n","        self.dropout2 = nn.Dropout(0.5)\n","        self.fc1 = nn.Linear(128 * 7 * 7, 512)\n","        self.fc2 = nn.Linear(512, num_classes)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv3(x))\n","        x = self.dropout1(x)\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout2(x)\n","        x = self.fc2(x)\n","        return x\n","\n","    def extract_features(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv3(x))\n","        x = self.dropout1(x)\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        return x\n","\n","# CNN Feature Extractor (frozen)\n","class CNNFeatureExtractor(nn.Module):\n","    def __init__(self, pretrained_cnn_path, feature_dim=512):\n","        super(CNNFeatureExtractor, self).__init__()\n","        self.cnn = SimpleCNN()\n","        self.cnn.load_state_dict(torch.load(pretrained_cnn_path, map_location=device))\n","        for param in self.cnn.parameters():\n","            param.requires_grad = False\n","        self.cnn.eval()\n","        self.feature_dim = feature_dim\n","\n","    def forward(self, x):\n","        with torch.no_grad():\n","            features = self.cnn.extract_features(x)\n","        return features\n","\n","# Visual Memory Model\n","class VisualMemoryModel(nn.Module):\n","    def __init__(self, pretrained_cnn_path, rnn_hidden_dim=256,\n","                 projection_dim=128, rnn_type='LSTM', num_layers=2, dropout=0.3):\n","        super(VisualMemoryModel, self).__init__()\n","\n","        self.cnn_features = CNNFeatureExtractor(pretrained_cnn_path)\n","        cnn_feature_dim = self.cnn_features.feature_dim\n","\n","        self.feature_projection = nn.Sequential(\n","            nn.Linear(cnn_feature_dim, projection_dim * 2),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(projection_dim * 2, projection_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout * 0.5)\n","        )\n","\n","        self.rnn_type = rnn_type\n","        if rnn_type == 'LSTM':\n","            self.rnn = nn.LSTM(\n","                projection_dim, rnn_hidden_dim,\n","                num_layers=num_layers,\n","                batch_first=True,\n","                dropout=dropout if num_layers > 1 else 0\n","            )\n","        elif rnn_type == 'GRU':\n","            self.rnn = nn.GRU(\n","                projection_dim, rnn_hidden_dim,\n","                num_layers=num_layers,\n","                batch_first=True,\n","                dropout=dropout if num_layers > 1 else 0\n","            )\n","\n","        self.memory_classifier = nn.Sequential(\n","            nn.Linear(rnn_hidden_dim, rnn_hidden_dim // 2),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(rnn_hidden_dim // 2, 64),\n","            nn.ReLU(),\n","            nn.Dropout(dropout * 0.5),\n","            nn.Linear(64, 2)\n","        )\n","\n","        self.rnn_hidden_dim = rnn_hidden_dim\n","        self.num_layers = num_layers\n","        self.projection_dim = projection_dim\n","\n","    def forward(self, x):\n","        batch_size, seq_len = x.size(0), x.size(1)\n","        x = x.view(batch_size * seq_len, *x.shape[2:])\n","        cnn_features = self.cnn_features(x)\n","        projected_features = self.feature_projection(cnn_features)\n","        projected_features = projected_features.view(batch_size, seq_len, -1)\n","\n","        if self.rnn_type == 'LSTM':\n","            rnn_output, (hidden, cell) = self.rnn(projected_features)\n","        else:\n","            rnn_output, hidden = self.rnn(projected_features)\n","\n","        final_output = rnn_output[:, -1, :]\n","        logits = self.memory_classifier(final_output)\n","        return logits\n","\n","    def get_hidden_states(self, x):\n","        batch_size, seq_len = x.size(0), x.size(1)\n","        x = x.view(batch_size * seq_len, *x.shape[2:])\n","        cnn_features = self.cnn_features(x)\n","        projected_features = self.feature_projection(cnn_features)\n","        projected_features = projected_features.view(batch_size, seq_len, -1)\n","\n","        if self.rnn_type == 'LSTM':\n","            rnn_output, _ = self.rnn(projected_features)\n","        else:\n","            rnn_output, _ = self.rnn(projected_features)\n","\n","        return rnn_output, projected_features\n","\n","print(\"‚úÖ Model architecture loaded!\")"],"metadata":{"id":"Idu9Y9EG66dx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class VanillaRNNVisualMemoryModel(nn.Module):\n","    def __init__(self, pretrained_cnn_path, rnn_hidden_dim=256,\n","                 projection_dim=128, num_layers=2, dropout=0.3):\n","        super(VanillaRNNVisualMemoryModel, self).__init__()\n","\n","        # Frozen CNN feature extractor (same as before)\n","        self.cnn_features = CNNFeatureExtractor(pretrained_cnn_path)\n","        cnn_feature_dim = self.cnn_features.feature_dim  # 512\n","\n","        # Trainable feature projection\n","        self.feature_projection = nn.Sequential(\n","            nn.Linear(cnn_feature_dim, projection_dim * 2),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(projection_dim * 2, projection_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout * 0.5)\n","        )\n","\n","        # VANILLA RNN (key difference!)\n","        self.rnn_type = 'Vanilla'\n","        self.num_layers = num_layers\n","        self.rnn_hidden_dim = rnn_hidden_dim\n","\n","        # Stack of vanilla RNN layers\n","        self.rnn_layers = nn.ModuleList()\n","\n","        # First layer: input_size = projection_dim\n","        self.rnn_layers.append(\n","            nn.RNN(projection_dim, rnn_hidden_dim, num_layers=1,\n","                   batch_first=True, nonlinearity='tanh')\n","        )\n","\n","        # Additional layers (if num_layers > 1)\n","        for _ in range(num_layers - 1):\n","            self.rnn_layers.append(\n","                nn.RNN(rnn_hidden_dim, rnn_hidden_dim, num_layers=1,\n","                       batch_first=True, nonlinearity='tanh')\n","            )\n","\n","        # Dropout between layers\n","        self.rnn_dropout = nn.Dropout(dropout)\n","\n","        # Memory comparison and classification head\n","        self.memory_classifier = nn.Sequential(\n","            nn.Linear(rnn_hidden_dim, rnn_hidden_dim // 2),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(rnn_hidden_dim // 2, 64),\n","            nn.ReLU(),\n","            nn.Dropout(dropout * 0.5),\n","            nn.Linear(64, 2)  # Binary: match (1) vs no-match (0)\n","        )\n","\n","        self.projection_dim = projection_dim\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: [batch_size, seq_len, channels, height, width]\n","        Returns:\n","            output: [batch_size, 2] logits for match/no-match classification\n","        \"\"\"\n","        batch_size, seq_len = x.size(0), x.size(1)\n","\n","        # Reshape to process all images at once\n","        x = x.view(batch_size * seq_len, *x.shape[2:])\n","\n","        # Extract CNN features (frozen)\n","        cnn_features = self.cnn_features(x)  # [batch_size * seq_len, 512]\n","\n","        # Project to lower dimension (trainable)\n","        projected_features = self.feature_projection(cnn_features)  # [batch_size * seq_len, projection_dim]\n","\n","        # Reshape for RNN processing\n","        projected_features = projected_features.view(batch_size, seq_len, -1)  # [batch_size, seq_len, projection_dim]\n","\n","        # Process through stacked vanilla RNN layers\n","        rnn_input = projected_features\n","\n","        for i, rnn_layer in enumerate(self.rnn_layers):\n","            rnn_output, hidden = rnn_layer(rnn_input)\n","\n","            # Apply dropout between layers (except last layer)\n","            if i < len(self.rnn_layers) - 1:\n","                rnn_output = self.rnn_dropout(rnn_output)\n","\n","            rnn_input = rnn_output\n","\n","        # Use the final output for classification\n","        final_output = rnn_output[:, -1, :]  # [batch_size, rnn_hidden_dim]\n","\n","        # Binary classification: match vs no-match\n","        logits = self.memory_classifier(final_output)  # [batch_size, 2]\n","\n","        return logits\n","\n","    def get_hidden_states(self, x):\n","        \"\"\"Get hidden states at each timestep for analysis\"\"\"\n","        batch_size, seq_len = x.size(0), x.size(1)\n","\n","        x = x.view(batch_size * seq_len, *x.shape[2:])\n","        cnn_features = self.cnn_features(x)\n","        projected_features = self.feature_projection(cnn_features)\n","        projected_features = projected_features.view(batch_size, seq_len, -1)\n","\n","        # Process through RNN layers to get final hidden states\n","        rnn_input = projected_features\n","\n","        for rnn_layer in self.rnn_layers:\n","            rnn_output, _ = rnn_layer(rnn_input)\n","            rnn_input = rnn_output\n","\n","        return rnn_output, projected_features"],"metadata":{"id":"ajgeANxE0vR5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_model_accuracy(model, test_loader, device):\n","        \"\"\"Test overall model accuracy\"\"\"\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        match_correct = 0\n","        match_total = 0\n","        nomatch_correct = 0\n","        nomatch_total = 0\n","\n","        all_predictions = []\n","        all_targets = []\n","        all_confidences = []\n","\n","        with torch.no_grad():\n","            for sequences, targets in tqdm(test_loader, desc=\"Testing\"):\n","                sequences, targets = sequences.to(device), targets.to(device)\n","                outputs = model(sequences)\n","                probabilities = F.softmax(outputs, dim=1)\n","                _, predicted = outputs.max(1)\n","\n","                total += targets.size(0)\n","                correct += predicted.eq(targets).sum().item()\n","\n","                # Store predictions and confidences\n","                all_predictions.extend(predicted.cpu().numpy())\n","                all_targets.extend(targets.cpu().numpy())\n","                all_confidences.extend(probabilities.max(1)[0].cpu().numpy())\n","\n","                # Separate accuracy for match vs no-match\n","                match_mask = (targets == 1)\n","                nomatch_mask = (targets == 0)\n","\n","                if match_mask.sum() > 0:\n","                    match_correct += predicted[match_mask].eq(targets[match_mask]).sum().item()\n","                    match_total += match_mask.sum().item()\n","\n","                if nomatch_mask.sum() > 0:\n","                    nomatch_correct += predicted[nomatch_mask].eq(targets[nomatch_mask]).sum().item()\n","                    nomatch_total += nomatch_mask.sum().item()\n","\n","        overall_acc = 100. * correct / total\n","        match_acc = 100. * match_correct / max(match_total, 1)\n","        nomatch_acc = 100. * nomatch_correct / max(nomatch_total, 1)\n","        avg_confidence = np.mean(all_confidences)\n","\n","        return {\n","            'overall_accuracy': overall_acc/100,\n","            'match_accuracy': match_acc,\n","            'nomatch_accuracy': nomatch_acc,\n","            'total_samples': total,\n","            'match_samples': match_total,\n","            'nomatch_samples': nomatch_total,\n","            'average_confidence': avg_confidence,\n","            'predictions': all_predictions,\n","            'targets': all_targets,\n","            'confidences': all_confidences\n","        }"],"metadata":{"id":"LgCD4ivmfWPY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import glob\n","import pickle\n","import matplotlib.pyplot as plt\n","import torch\n","from torch.utils.data import DataLoader\n","\n","data_num = [1, 2, 3, 5, 7, 10]\n","PRETRAINED_CNN_PATH = 'small_cnn_model.pth'\n","\n","# Dictionary to store results\n","results_dict = {}\n","accuracy_scores = []\n","noise_levels = []\n","\n","# Loop through each number in data_num\n","for num in data_num:\n","    print(f\"\\n{'='*50}\")\n","    print(f\"Testing with noise level: {num}\")\n","    print(f\"{'='*50}\")\n","\n","    # Define paths for current iteration\n","    VISUAL_MEMORY_MODEL_PATH = f'./RNN_model/visual_memory_Vanilla_noise{num}.pth'\n","    TEST_DATASET_PATH = f'./my_datasets/test_1000samples_*{num}dist_*.pkl'\n","\n","    # Check if model exists\n","    try:\n","\n","        model = VanillaRNNVisualMemoryModel(\n","              pretrained_cnn_path=PRETRAINED_CNN_PATH,\n","              rnn_hidden_dim=256,\n","              projection_dim=128,\n","              num_layers=2,\n","              dropout=0.3\n","          ).to(device)\n","        \"\"\"\n","        model = VisualMemoryModel(\n","            pretrained_cnn_path=PRETRAINED_CNN_PATH,\n","            rnn_hidden_dim=256,\n","            projection_dim=128,\n","            rnn_type='LSTM',  # Note: You mentioned GRU but your model files are named GRU_noise*\n","            num_layers=2,\n","            dropout=0.3\n","        ).to(device)\n","        \"\"\"\n","        model.load_state_dict(torch.load(VISUAL_MEMORY_MODEL_PATH, map_location=device))\n","        model.eval()\n","        print(f\"‚úÖ Model loaded: {VISUAL_MEMORY_MODEL_PATH}\")\n","\n","    except FileNotFoundError:\n","        print(f\"‚ùå Model not found: {VISUAL_MEMORY_MODEL_PATH}\")\n","        continue\n","    except Exception as e:\n","        print(f\"‚ùå Error loading model: {e}\")\n","        continue\n","\n","    # Find and load test dataset\n","    test_files = glob.glob(TEST_DATASET_PATH)\n","\n","    if test_files:\n","        test_path = test_files[0]  # Use the first matching file\n","        print(f\"üìÅ Loading test dataset from: {test_path}\")\n","\n","        try:\n","            with open(test_path, 'rb') as f:\n","                test_data = pickle.load(f)\n","\n","            # Create simple dataset class\n","            class SimpleTestDataset:\n","                def __init__(self, data_dict):\n","                    if isinstance(data_dict, dict):\n","                        self.samples = data_dict['samples']\n","                        self.labels = data_dict['labels']\n","                        self.metadata = data_dict['metadata']\n","                        self.num_distractors = data_dict.get('num_distractors', num)\n","                    else:\n","                        # If it's the dataset object itself\n","                        self.samples = data_dict.samples\n","                        self.labels = data_dict.labels\n","                        self.metadata = data_dict.metadata\n","                        self.num_distractors = data_dict.num_distractors\n","\n","                def __len__(self):\n","                    return len(self.samples)\n","\n","                def __getitem__(self, idx):\n","                    return self.samples[idx], self.labels[idx]\n","\n","                def get_sample_with_metadata(self, idx):\n","                    return self.samples[idx], self.labels[idx], self.metadata[idx]\n","\n","            test_dataset = SimpleTestDataset(test_data)\n","            print(f\"‚úÖ Test dataset loaded: {len(test_dataset)} samples\")\n","\n","            # Create data loader\n","            test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","            # Test model and get results\n","            results = test_model_accuracy(model, test_loader, device)\n","            overall_accuracy = results['overall_accuracy']\n","\n","            # Store results\n","            results_dict[num] = {\n","                'overall_accuracy': overall_accuracy,\n","                'full_results': results\n","            }\n","            accuracy_scores.append(overall_accuracy)\n","            noise_levels.append(num)\n","\n","            print(f\"üéØ Overall Accuracy for noise {num}: {overall_accuracy:.4f}\")\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error processing dataset: {e}\")\n","            continue\n","\n","    else:\n","        print(f\"‚ùå No test dataset found matching: {TEST_DATASET_PATH}\")\n","        continue\n","\n","'''\n","# Print summary of results\n","print(f\"\\n{'='*60}\")\n","print(\"SUMMARY OF RESULTS\")\n","print(f\"{'='*60}\")\n","for num in data_num:\n","    if num in results_dict:\n","        acc = results_dict[num]['overall_accuracy']\n","        print(f\"Noise {num:2d}: {acc:.4f} ({acc*100:.2f}%)\")\n","    else:\n","        print(f\"Noise {num:2d}: No results (missing files)\")\n","\n","# Plot results\n","if accuracy_scores and noise_levels:\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(noise_levels, accuracy_scores, 'bo-', linewidth=2, markersize=8)\n","    plt.xlabel('Number of Distractors (Noise Level)', fontsize=12)\n","    plt.ylabel('Overall Accuracy', fontsize=12)\n","    plt.title('Model Performance along with Number of Distractors', fontsize=14)\n","    plt.grid(True, alpha=0.3)\n","    plt.ylim(0.5, 1)\n","    plt.xticks(noise_levels)\n","\n","    # Add value labels on points\n","    for i, (x, y) in enumerate(zip(noise_levels, accuracy_scores)):\n","        plt.annotate(f'{y:.3f}', (x, y), textcoords=\"offset points\",\n","                    xytext=(0,10), ha='center', fontsize=10)\n","\n","    plt.tight_layout()\n","    #plt.savefig('model_accuracy_vs_noise.png', dpi=300, bbox_inches='tight')\n","    plt.show()\n","\n","    print(f\"\\nüìä Plot saved as 'model_accuracy_vs_noise.png'\")\n","else:\n","    print(\"\\n‚ùå No results to plot - no models were successfully tested\")\n","'''"],"metadata":{"id":"fO9YWmajeuLO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Get RDM correlation"],"metadata":{"id":"-TkqQz0SHCxZ"}},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from sklearn.metrics.pairwise import cosine_similarity\n","from scipy.spatial.distance import pdist, squareform\n","from scipy.cluster.hierarchy import dendrogram, linkage\n","from tqdm import tqdm\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","def extract_hidden_states_dataset(model, dataset, device, num_samples=100):\n","    \"\"\"Extract hidden states for multiple samples to analyze geometry\"\"\"\n","\n","    model.eval()\n","    all_hidden_states = []\n","    all_metadata = []\n","\n","    print(f\"üîç Extracting hidden states from {num_samples} samples...\")\n","\n","    with torch.no_grad():\n","        for i in tqdm(range(min(num_samples, len(dataset)))):\n","            sequence, label = dataset[i]\n","\n","            try:\n","                _, _, metadata = dataset.get_sample_with_metadata(i)\n","                target_digit = metadata['target_digit']\n","                probe_digit = metadata['probe_digit']\n","                is_match = metadata['is_match']\n","            except:\n","                target_digit = -1\n","                probe_digit = -1\n","                is_match = (label == 1)\n","\n","            # Get hidden states for this sequence\n","            sequence_batch = sequence.unsqueeze(0).to(device)\n","            hidden_states, projected_features = model.get_hidden_states(sequence_batch)\n","\n","            # Store results\n","            hidden_np = hidden_states[0].cpu().numpy()  # [seq_len, hidden_dim]\n","            projected_np = projected_features[0].cpu().numpy()  # [seq_len, proj_dim]\n","\n","            sample_data = {\n","                'sample_idx': i,\n","                'target_digit': target_digit,\n","                'probe_digit': probe_digit,\n","                'is_match': is_match,\n","                'true_label': label,\n","                'hidden_states': hidden_np,  # [5, 256]\n","                'projected_features': projected_np,  # [5, 128]\n","                'sequence': sequence.numpy()\n","            }\n","\n","            all_hidden_states.append(sample_data)\n","            all_metadata.append({\n","                'sample_idx': i,\n","                'target_digit': target_digit,\n","                'probe_digit': probe_digit,\n","                'is_match': is_match\n","            })\n","\n","    print(f\"‚úÖ Extracted hidden states for {len(all_hidden_states)} samples\")\n","    return all_hidden_states, all_metadata"],"metadata":{"id":"b22oAgM-HB86"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","import glob\n","import pickle\n","import matplotlib.pyplot as plt\n","import torch\n","from torch.utils.data import DataLoader\n","\n","import torch\n","import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics.pairwise import cosine_similarity\n","from scipy.spatial.distance import pdist, squareform\n","from scipy.stats import spearmanr\n","from scipy.stats import pearsonr\n","import warnings\n","\n","data_num = [1, 2, 3, 5, 7, 10]\n","PRETRAINED_CNN_PATH = 'small_cnn_model.pth'\n","\n","# Dictionary to store results\n","results_dict = {}\n","accuracy_scores = []\n","noise_levels = []\n","\n","# Loop through each number in data_num\n","for num in data_num:\n","    print(f\"\\n{'='*50}\")\n","    print(f\"Testing with noise level: {num}\")\n","    print(f\"{'='*50}\")\n","\n","    # Define paths for current iteration\n","    VISUAL_MEMORY_MODEL_PATH = f'./RNN_model/visual_memory_Vanilla_noise{num}.pth'\n","    TEST_DATASET_PATH = f'./my_datasets/test_1000samples_*{num}dist_*.pkl'\n","\n","    # Check if model exists\n","    try:\n","        # Load model\n","        \"\"\"\n","        model = VisualMemoryModel(\n","            pretrained_cnn_path=PRETRAINED_CNN_PATH,\n","            rnn_hidden_dim=256,\n","            projection_dim=128,\n","            rnn_type='LSTM',  # Note: You mentioned GRU but your model files are named GRU_noise*\n","            num_layers=2,\n","            dropout=0.3\n","        ).to(device)\n","        \"\"\"\n","        model = VanillaRNNVisualMemoryModel(\n","              pretrained_cnn_path=PRETRAINED_CNN_PATH,\n","              rnn_hidden_dim=256,\n","              projection_dim=128,\n","              num_layers=2,\n","              dropout=0.3\n","          ).to(device)\n","\n","        model.load_state_dict(torch.load(VISUAL_MEMORY_MODEL_PATH, map_location=device))\n","        model.eval()\n","        print(f\"‚úÖ Model loaded: {VISUAL_MEMORY_MODEL_PATH}\")\n","\n","    except FileNotFoundError:\n","        print(f\"‚ùå Model not found: {VISUAL_MEMORY_MODEL_PATH}\")\n","        continue\n","    except Exception as e:\n","        print(f\"‚ùå Error loading model: {e}\")\n","        continue\n","\n","    # Find and load test dataset\n","    test_files = glob.glob(TEST_DATASET_PATH)\n","\n","    if test_files:\n","        test_path = test_files[0]  # Use the first matching file\n","        print(f\"üìÅ Loading test dataset from: {test_path}\")\n","\n","        try:\n","            with open(test_path, 'rb') as f:\n","                test_data = pickle.load(f)\n","\n","            # Create simple dataset class\n","            class SimpleTestDataset:\n","                def __init__(self, data_dict):\n","                    if isinstance(data_dict, dict):\n","                        self.samples = data_dict['samples']\n","                        self.labels = data_dict['labels']\n","                        self.metadata = data_dict['metadata']\n","                        self.num_distractors = data_dict.get('num_distractors', num)\n","                    else:\n","                        # If it's the dataset object itself\n","                        self.samples = data_dict.samples\n","                        self.labels = data_dict.labels\n","                        self.metadata = data_dict.metadata\n","                        self.num_distractors = data_dict.num_distractors\n","\n","                def __len__(self):\n","                    return len(self.samples)\n","\n","                def __getitem__(self, idx):\n","                    return self.samples[idx], self.labels[idx]\n","\n","                def get_sample_with_metadata(self, idx):\n","                    return self.samples[idx], self.labels[idx], self.metadata[idx]\n","\n","            test_dataset = SimpleTestDataset(test_data)\n","            print(f\"‚úÖ Test dataset loaded: {len(test_dataset)} samples\")\n","\n","            # Create data loader\n","            test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","            hidden_states_data, metadata = extract_hidden_states_dataset(\n","                    model, test_dataset, device, 1000\n","                )\n","            digit_states = {}\n","            digit_states['Target'] = {}\n","            digit_states['Probe'] = {}\n","            for digit in range(10):\n","              digit_samples = [d for d in hidden_states_data if d['target_digit'] == digit]\n","              if len(digit_samples) > 0:\n","                  # Collect all states for this digit at this timestep\n","                  states = [sample['hidden_states'][0] for sample in digit_samples]\n","                  digit_states['Target'][digit] = np.array(states)\n","\n","                  states = [sample['hidden_states'][-1] for sample in digit_samples]\n","                  digit_states['Probe'][digit] = np.array(states)\n","            timestep_names = ['Target', 'Probe']\n","            rdms = {}\n","            for idx, timestep_name in enumerate(timestep_names):\n","                    if idx >= 6:\n","                        break\n","\n","                    # Get available digits for this timestep\n","                    available_digits = sorted([d for d in digit_states[timestep_name].keys()\n","                                            if len(digit_states[timestep_name][d]) > 0])\n","\n","                    if len(available_digits) < 2:\n","                        continue\n","\n","                    # Calculate average representation for each digit\n","                    avg_representations = []\n","                    digit_labels = []\n","\n","                    for digit in available_digits:\n","                        states = digit_states[timestep_name][digit]\n","                        avg_state = np.mean(states, axis=0)\n","                        avg_representations.append(avg_state)\n","                        digit_labels.append(digit)\n","\n","                    avg_representations = np.array(avg_representations)  # [n_digits, hidden_dim]\n","\n","\n","\n","                        # Cosine dissimilarity = 1 - cosine similarity\n","                    similarity_matrix = cosine_similarity(avg_representations)\n","                    rdm = 1 - similarity_matrix\n","                    rdms[timestep_name] = rdm\n","\n","            t1 = 'Target'\n","            t2 = 'Probe'\n","            rdm1_upper = rdms[t1][np.triu_indices_from(rdms[t1], k=1)]\n","            rdm2_upper = rdms[t2][np.triu_indices_from(rdms[t2], k=1)]\n","\n","            if len(rdm1_upper) > 0 and len(rdm2_upper) > 0:\n","                corr, _ = spearmanr(rdm1_upper, rdm2_upper)\n","\n","            print(corr)\n","            results_dict[num] = {\n","                'overall_accuracy': corr,\n","                'full_results': results\n","            }\n","            accuracy_scores.append(corr)\n","            noise_levels.append(num)\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error processing dataset: {e}\")\n","            continue\n","\n","    else:\n","        print(f\"‚ùå No test dataset found matching: {TEST_DATASET_PATH}\")\n","        continue"],"metadata":{"id":"ZxPt9VgVG_ET"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"\\n{'='*60}\")\n","print(\"SUMMARY OF RESULTS\")\n","print(f\"{'='*60}\")\n","for num in data_num:\n","    if num in results_dict:\n","        acc = results_dict[num]['overall_accuracy']\n","        print(f\"Noise {num:2d}: {acc:.4f} ({acc*100:.2f}%)\")\n","    else:\n","        print(f\"Noise {num:2d}: No results (missing files)\")\n","\n","# Plot results\n","if accuracy_scores and noise_levels:\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(noise_levels, accuracy_scores, 'bo-', linewidth=2, markersize=8)\n","    plt.xlabel('Number of Distractors (Noise Level)', fontsize=12)\n","    plt.ylabel('Correlation Value', fontsize=12)\n","    plt.title('Correlation between Target RDM and Probe RDM', fontsize=14)\n","    plt.grid(True, alpha=0.3)\n","    plt.ylim(0.5, 1)\n","    plt.xticks(noise_levels)\n","    # Add value labels on points\n","    for i, (x, y) in enumerate(zip(noise_levels, accuracy_scores)):\n","        plt.annotate(f'{y:.3f}', (x, y), textcoords=\"offset points\",\n","                    xytext=(0,10), ha='center', fontsize=10)\n","\n","    plt.tight_layout()\n","    #plt.savefig('model_accuracy_vs_noise.png', dpi=300, bbox_inches='tight')\n","    plt.show()"],"metadata":{"id":"J1b1b1w6K6HR"},"execution_count":null,"outputs":[]}]}