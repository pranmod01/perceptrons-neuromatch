{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aec057e",
   "metadata": {},
   "source": [
    "# SNN Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "4f124e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Version: 3.9.18 (main, Sep 11 2023, 13:30:38) [MSC v.1916 64 bit (AMD64)]\n",
      "PyTorch version 2.3.1+cu118\n",
      "Numpy version 1.26.4\n",
      "env path:  c:\\Users\\richa\\OneDrive\\Dugree\\Project\\cuda\\Scripts\\python.exe\n",
      "Using device:  cuda\n",
      "       --> 0 : NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Module\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "print('System Version:', sys.version)\n",
    "print('PyTorch version', torch.__version__)\n",
    "print('Numpy version', np.__version__)\n",
    "print(\"env path: \", sys.executable) \n",
    "\n",
    "#[+] check to see if gpu is available, else use cpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('Using device: ',device)\n",
    "for i in range(torch.cuda.device_count()): print('       -->',i,':', torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34edb18a",
   "metadata": {},
   "source": [
    "## DataLoading MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9bd2a728",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 128\n",
    "data_path= r'./data'\n",
    "dtype= torch.float\n",
    "\n",
    "# Create the transoform for MNIST dataset to make sure its 28x28, grayscale, a tensor, and vals normalized to fall between 0 and 1\n",
    "transform= transforms.Compose([\n",
    "    transforms.Resize((28,28)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0,), (1,)),])\n",
    "\n",
    "# Automatically downloads and splits the MNIST dataset\n",
    "mnist_train = datasets.MNIST(data_path, train= True , download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train= False, download=True, transform=transform)\n",
    "\n",
    "#create DataLoaders\n",
    "train_loader= DataLoader(mnist_train , batch_size= batch_size, shuffle=True, drop_last=True)\n",
    "test_loader= DataLoader(mnist_test , batch_size= batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeb1eed",
   "metadata": {},
   "source": [
    "# Construct a Fully Connected SNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "bf1909b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of inputs should match number of pixels in the MNIST img\n",
    "num_inputs= 28*28  #= 784\n",
    "\n",
    "# Hidden layer is however big you want as long as it fits in your GPU\n",
    "num_hidden= 1000\n",
    "\n",
    "# One output neuron for each of the 10 MNIST digits\n",
    "num_outputs= 10\n",
    "\n",
    "# 25 time steps is a quick simulation\n",
    "num_steps= 25\n",
    "\n",
    "# Rate of decay\n",
    "beta= 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d4072903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Snn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers before defining the forward function\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # init hidden states at t=0, mem is membrane potential\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        \n",
    "        # [+] Record the hidden layer\n",
    "        spk1_rec = []\n",
    "        mem1_rec = []\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            # x.shape: torch.Size([128, 784]) -> 128=batch_size, 784= 28x28 img; x is a batch of 128 images\n",
    "\n",
    "            # cur1 , spk1, mem1 shape:  torch.Size([128, 1000]) torch.Size([128, 1000]) torch.Size([128, 1000])\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            \n",
    "            # cur2 , spk2, mem2 shape:  torch.Size([128, 10]) torch.Size([128, 10]) torch.Size([128, 10])\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            \n",
    "            # [+] store hidden layer in list\n",
    "            spk1_rec.append(spk1)\n",
    "            mem1_rec.append(mem1)\n",
    "\n",
    "            # stor final layer in list\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        # rec stack sizes:  torch.Size([25, 128, 10]) torch.Size([25, 128, 10])\n",
    "        spk_stack, mem_stack= torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "        spk_stack_hid, mem_stack_hid= torch.stack(spk1_rec, dim=0), torch.stack(mem1_rec, dim=0)\n",
    "\n",
    "        # The network returns a tensor of spike recordings over time, and a tensor of membrane potential recordnigs over time\n",
    "        return spk_stack, mem_stack, spk_stack_hid, mem_stack_hid\n",
    "\n",
    "        \n",
    "# Load the network onto CUDA if available\n",
    "snn = Snn().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bff27f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648cf923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t Train Loss: 2.643714427947998\n",
      "Iteration: 10 \t Train Loss: 0.771937370300293\n",
      "Iteration: 20 \t Train Loss: 0.5643541812896729\n",
      "Iteration: 30 \t Train Loss: 0.4867444634437561\n",
      "Iteration: 40 \t Train Loss: 0.5025603771209717\n",
      "Iteration: 50 \t Train Loss: 0.7258651852607727\n",
      "Iteration: 60 \t Train Loss: 0.5053210258483887\n",
      "Iteration: 70 \t Train Loss: 0.4704912304878235\n",
      "Iteration: 80 \t Train Loss: 0.523330569267273\n",
      "Iteration: 90 \t Train Loss: 0.4312777817249298\n",
      " -->[+] model saved\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(snn.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "\n",
    "# # 60000 data samples / 128 samples per batch = approx 468 iterations\n",
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "cntLim= 100\n",
    "saveMdl= False\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    # Minibatch training loop: each batch will have data which is the 128 samples, and each sample will have target labels (digits 0-9), we load all into cuda\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        #forward pass: we set our network to train mode, and pass the data into it\n",
    "        snn.train()\n",
    "        spk_rec, mem_rec, _, _ = snn(data.flatten(1))\n",
    "        #spk_rec and mem_rec sizes:  torch.Size([25, 128, 10]) \n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "        loss_val += loss(spk_rec.sum(0), targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Print train/test loss/accuracy\n",
    "        if counter % 10 == 0:\n",
    "            print(f\"Iteration: {counter} \\t Train Loss: {loss_val.item()}\")\n",
    "        counter += 1\n",
    "\n",
    "        if counter == cntLim:\n",
    "            if(saveMdl):\n",
    "                torch.save(snn.state_dict(), 'snn_mdl.pth')\n",
    "                print(' -->[+] model saved')\n",
    "            else: print('   -->[x] no save')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a9481",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "74bc40d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_accuracy(model, dataloader):\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    running_length = 0\n",
    "    running_accuracy = 0\n",
    "\n",
    "    for data, targets in iter(dataloader):\n",
    "      # data shape:  torch.Size([128, 1, 28, 28])\n",
    "      data = data.to(device)\n",
    "      targets = targets.to(device)\n",
    "\n",
    "      # flat data shape:  torch.Size([128, 784]) -> a batch of 28x28 imsg is flattened to a batch of 1x784 along dim 1\n",
    "      data= data.flatten(1)\n",
    "\n",
    "      # spk_rec, mem_rec = model(data)\n",
    "      spk_rec, mem_rec, spk_stack_hid, mem_stack_hid = model(data)\n",
    "      \n",
    "      spike_count = spk_rec.sum(0)\n",
    "      _, max_spike = spike_count.max(1)\n",
    "\n",
    "      # correct classes for one batch\n",
    "      num_correct = (max_spike == targets).sum()\n",
    "\n",
    "      # total accuracy\n",
    "      running_length += len(targets)\n",
    "      running_accuracy += num_correct\n",
    "    \n",
    "    accuracy = (running_accuracy / running_length)\n",
    "\n",
    "    return accuracy.item(), spk_rec, mem_rec, spk_stack_hid, mem_stack_hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "6ccc38e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spk_rec:  torch.Size([25, 128, 10]) \n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0') \n",
      "\n",
      "mem_rec:  torch.Size([25, 128, 10]) \n",
      " tensor([-7.9081, -1.1569, -1.1815,  0.7796, -1.4186, -9.8872, -1.7960, -5.5927,\n",
      "         1.1594, -5.7758], device='cuda:0') \n",
      "\n",
      "spk_stack_hid:  torch.Size([25, 128, 1000]) \n",
      " tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') \n",
      "\n",
      "mem_stack_hid:  torch.Size([25, 128, 1000]) \n",
      " tensor([-1.9087e+00,  6.7174e-01, -6.4489e+00,  1.0788e+00,  6.0000e-01,\n",
      "        -2.0474e+00, -7.3808e+00,  7.5470e-01,  3.4369e-01, -4.3363e+00,\n",
      "        -6.3369e+00, -8.7778e+00, -5.5969e-01, -5.8716e+00,  7.1615e-01,\n",
      "        -1.7765e+00, -4.2597e+00,  7.1902e-01,  8.1472e-01,  4.3086e-01,\n",
      "         4.1539e-01, -3.8685e+00, -6.6031e+00,  6.3289e-01, -5.2781e+00,\n",
      "        -1.1811e+01, -1.1593e+01, -2.1673e-01, -1.1960e+01,  5.1034e-01,\n",
      "         9.7850e-01, -6.3211e+00, -1.7549e+00, -5.9270e-01, -2.0547e+00,\n",
      "         9.2547e-01, -1.4159e+00,  3.4922e-01, -2.9478e+00, -2.8543e+00,\n",
      "        -1.6059e+00, -8.6650e+00,  1.4023e+00,  6.5012e-01,  1.0443e+00,\n",
      "         1.3433e+00, -5.8563e+00,  1.2729e+00,  8.6590e-01, -4.0196e+00,\n",
      "        -1.0087e-01,  7.5838e-01,  1.0010e-01, -1.5341e+00,  9.6327e-01,\n",
      "        -2.4907e+00,  8.8719e-01, -5.5830e+00,  9.6191e-01, -2.8798e+00,\n",
      "         4.6631e-01,  7.6302e-01,  9.1631e-02,  6.0293e-01,  8.3061e-01,\n",
      "        -3.5657e+00,  4.5798e-01,  6.2875e-01,  6.0005e-01,  1.3724e+00,\n",
      "        -4.6247e+00,  1.1307e+00, -3.8936e+00, -5.3105e+00, -3.4195e+00,\n",
      "        -6.5962e+00,  7.2430e-01, -4.4325e+00, -2.4511e-01, -3.3374e+00,\n",
      "        -1.2574e+00, -5.0520e+00,  1.0454e+00,  1.0510e+00, -6.1642e+00,\n",
      "        -2.2786e+00, -2.8909e+00, -1.8875e+00, -3.3008e+00, -7.0328e+00,\n",
      "         8.4808e-01, -7.6312e+00, -5.0569e+00, -4.6758e+00, -7.3149e+00,\n",
      "         1.2036e+00,  6.2506e-01,  6.1411e-01,  9.2096e-01,  9.7297e-01,\n",
      "         4.6444e-01,  9.7851e-01, -7.9011e+00, -8.9686e+00, -1.9834e+00,\n",
      "        -5.0600e+00, -2.7517e+00, -5.1072e+00, -1.7655e+00, -3.8450e+00,\n",
      "         4.1774e-01, -7.9688e-01, -9.4012e+00, -9.1338e-01,  1.3225e+00,\n",
      "        -8.1305e+00, -1.8132e+00, -2.9087e+00, -4.1045e+00, -1.2792e+00,\n",
      "        -1.5477e+00, -3.8788e+00, -1.2105e+00, -1.0316e+01,  1.9009e-01,\n",
      "        -2.2748e+00, -3.9672e+00,  6.3209e-01,  1.1611e+00,  2.3855e-01,\n",
      "        -5.4551e+00, -1.0681e+01,  2.1002e-01, -1.3968e-01, -1.2214e+01,\n",
      "        -4.5422e+00, -7.8205e+00, -2.4896e+00,  1.3615e+00, -4.8161e+00,\n",
      "        -7.9920e-01,  8.7574e-01, -3.1532e+00,  6.4721e-01,  7.7865e-01,\n",
      "        -2.3745e+00, -2.6977e+00, -7.0642e-01, -3.8243e+00,  9.5041e-01,\n",
      "         8.4949e-01, -4.1524e+00,  3.3424e-01, -7.7730e+00,  1.0043e+00,\n",
      "         1.1119e-01,  2.4216e-01, -3.2428e-01,  9.9212e-01,  1.1245e+00,\n",
      "        -7.3867e-01,  4.0774e-01,  7.4291e-01,  8.9428e-01, -1.2859e+00,\n",
      "         1.0008e+00, -9.0698e+00, -7.8636e-02, -3.3264e+00, -3.5175e+00,\n",
      "        -1.3445e-01, -3.9048e-01,  2.2761e-01,  1.1974e+00,  2.1046e-01,\n",
      "         7.1760e-02,  2.7228e-01, -3.0898e+00, -2.0720e+00,  3.3900e-01,\n",
      "        -2.2217e+00, -3.7150e+00, -3.8658e+00, -1.3372e+00,  1.2454e+00,\n",
      "         7.8631e-01, -3.9133e+00, -9.2070e+00, -4.2462e-01,  5.7996e-01,\n",
      "        -2.6296e+00, -1.5449e+00,  7.6440e-01,  5.6136e-01,  7.2599e-01,\n",
      "        -2.3181e+00, -5.3383e+00, -6.4313e+00, -1.1301e+01, -5.8593e+00,\n",
      "         3.4296e-01,  1.3651e-01,  3.5912e-01, -5.9084e-01, -6.9241e+00,\n",
      "        -5.7531e+00,  9.8287e-01, -3.0415e+00,  4.2505e-01, -6.1832e+00,\n",
      "         6.8180e-01,  3.3603e-01,  4.0959e-01,  7.5735e-01,  9.1039e-01,\n",
      "         9.9758e-01, -3.0564e+00, -2.2087e+00, -6.8174e-02, -3.9694e+00,\n",
      "         5.8378e-01, -2.6862e+00, -4.9414e+00, -2.9481e+00,  3.6591e-01,\n",
      "         1.7916e-01, -2.3844e+00, -5.7408e+00, -1.0566e+00,  5.6417e-01,\n",
      "        -7.1689e-01, -2.0952e+00, -2.7678e+00,  4.9742e-01, -7.2977e+00,\n",
      "        -3.6477e+00, -2.5320e-01,  5.1436e-01, -1.5396e+00,  2.0292e-01,\n",
      "        -9.8815e-01,  1.2631e+00,  1.9192e-01,  4.0568e-01, -5.6047e+00,\n",
      "         1.0000e+00, -4.2018e+00, -5.6594e-01, -3.0960e+00, -3.4799e+00,\n",
      "         3.6622e-01, -5.6330e-01,  2.4575e-01,  1.0392e+00,  5.8711e-01,\n",
      "         9.9675e-01,  2.2520e-01,  7.8849e-01,  7.4763e-01, -1.6890e+00,\n",
      "         4.0191e-01,  1.3550e-01,  8.0783e-01,  2.2023e-01, -1.4061e+01,\n",
      "        -6.8551e+00, -9.9470e+00, -1.1639e+00,  7.0853e-01,  4.5372e-01,\n",
      "        -3.6220e+00,  1.0818e+00, -1.8637e+00, -3.1088e+00, -4.0848e+00,\n",
      "        -2.2657e+00, -4.5477e+00, -6.2725e+00,  8.5007e-01,  7.1122e-01,\n",
      "        -3.3212e+00,  8.4979e-01,  6.2049e-01,  2.9765e-01,  6.7296e-01,\n",
      "        -4.1028e+00,  4.2549e-01,  4.8976e-01,  3.0332e-01, -6.3202e-01,\n",
      "         4.7263e-01, -9.3772e-01,  1.6423e-01, -2.3096e+00, -9.2711e+00,\n",
      "         5.7123e-02, -6.2974e+00, -1.1091e+01, -1.0707e+00, -2.9125e+00,\n",
      "        -3.8812e+00, -3.1358e+00,  6.2033e-01, -8.7459e+00, -1.3862e+00,\n",
      "        -2.6940e+00, -8.0482e+00, -6.1295e+00,  1.3793e-01, -8.2697e+00,\n",
      "         5.3119e-01,  8.8410e-01, -7.6754e+00, -3.7503e+00, -4.3484e+00,\n",
      "         1.0401e-01,  6.4776e-01, -6.3408e+00, -8.3841e-01, -1.6619e+00,\n",
      "        -1.3781e+00,  7.7814e-01, -4.6187e-01, -7.1507e+00, -3.7106e+00,\n",
      "        -5.4828e+00,  7.7570e-01,  9.1100e-01,  8.6216e-02, -2.7324e+00,\n",
      "        -7.6726e+00, -3.5166e+00,  1.2819e+00, -3.6754e+00, -2.3537e-01,\n",
      "        -3.7168e+00, -1.5748e+00,  1.1374e+00,  4.3287e-01, -5.2576e+00,\n",
      "         2.9626e-01, -2.3001e-01,  8.7769e-01,  9.8490e-01, -3.7874e+00,\n",
      "         1.2869e-02, -2.7284e+00, -1.5720e+01, -2.5217e+00, -5.5599e-02,\n",
      "        -3.2003e+00,  3.3956e-01, -1.4133e+00,  7.6795e-01, -4.0014e+00,\n",
      "        -5.8672e+00,  6.6140e-02,  1.0110e+00, -7.1820e-01,  8.3976e-01,\n",
      "         2.9156e-02, -2.2098e+00,  5.2213e-01,  8.8699e-01,  8.1955e-01,\n",
      "        -3.2452e-01, -1.4235e+00, -4.1615e+00, -3.5121e+00, -5.8279e+00,\n",
      "        -3.2816e+00,  8.1637e-01, -3.5124e+00, -3.2897e+00, -3.0746e+00,\n",
      "        -6.2684e+00,  6.2895e-01, -4.9831e+00, -4.3418e+00, -3.5898e+00,\n",
      "        -3.0867e+00,  1.0968e+00, -3.3196e+00, -1.6300e+01, -5.0145e+00,\n",
      "        -1.9379e+00,  6.4854e-01, -4.0113e+00,  8.2704e-01,  5.2795e-01,\n",
      "        -1.5540e+00,  5.3546e-01,  6.3876e-01, -3.5296e+00,  2.5439e-01,\n",
      "        -3.0618e+00, -6.9573e+00, -1.1230e+00,  8.2759e-01, -5.8169e+00,\n",
      "         6.3266e-01, -3.9213e+00, -1.9564e+00,  9.3581e-01, -2.6550e+00,\n",
      "        -2.4649e+00, -1.5962e-01, -7.1109e+00, -5.0621e-01,  1.9537e-01,\n",
      "         7.6448e-01, -9.4415e-01,  7.2713e-01, -8.3288e+00,  2.5679e-01,\n",
      "        -4.5132e+00, -1.5182e+00, -6.7946e-01, -2.7060e+00,  1.0359e+00,\n",
      "        -6.4576e-01, -4.9474e+00, -8.2174e+00,  9.8688e-01,  7.4281e-01,\n",
      "        -6.8335e-01, -3.3446e+00, -2.5397e+00,  3.5426e-01, -2.1133e+00,\n",
      "         1.3989e+00, -1.3081e+01, -2.2912e+00,  4.1479e-01, -1.1743e+00,\n",
      "        -8.8279e+00,  1.1742e+00,  5.3837e-01, -7.0389e+00, -4.1763e-01,\n",
      "         4.8617e-01, -4.7727e+00,  4.7545e-01, -5.7113e+00,  1.6410e-01,\n",
      "        -8.9703e-01, -6.3349e+00,  4.0735e-01, -1.5072e+00, -3.1574e+00,\n",
      "        -1.0149e+01, -5.8495e+00, -3.0799e+00,  8.7116e-01, -6.0105e+00,\n",
      "        -7.5176e+00, -1.3093e+00, -2.5185e+00, -8.1789e+00, -1.3299e+00,\n",
      "         8.3037e-01, -6.0022e+00, -6.5468e-01,  1.0051e+00, -1.1871e+00,\n",
      "        -1.9832e+00, -4.0230e+00,  9.0822e-01, -5.0480e-01, -4.1427e+00,\n",
      "        -2.1749e+00, -1.6446e+00,  9.4892e-01, -1.1230e+00, -1.0980e-01,\n",
      "         7.9149e-01, -1.1765e+01,  6.7016e-01,  7.9988e-01,  4.2137e-01,\n",
      "         6.6581e-01, -3.0339e+00, -6.1252e+00, -1.6284e+00, -6.1172e+00,\n",
      "        -3.2950e+00, -4.2726e+00,  5.0626e-01, -1.1311e+00, -2.8015e+00,\n",
      "         8.8487e-01, -3.5538e+00,  5.5877e-01,  8.3836e-01, -3.7288e+00,\n",
      "         7.3197e-01,  2.7362e-01,  8.4210e-01, -2.6959e+00, -4.7213e+00,\n",
      "        -5.6140e+00,  1.1058e+00,  7.8899e-01,  6.9800e-02, -8.3693e+00,\n",
      "         3.2368e-01, -1.0335e-01, -5.4030e+00, -6.6784e+00,  1.0669e+00,\n",
      "         5.0555e-01,  4.0061e-01,  9.0426e-01, -1.2768e+00,  6.1589e-01,\n",
      "         9.3169e-02,  4.3193e-01,  5.5408e-01,  2.0493e-01, -7.8989e+00,\n",
      "        -3.1969e+00, -2.4925e+00,  1.5372e+00, -3.7160e+00,  6.0688e-01,\n",
      "        -6.5614e+00,  7.1783e-01,  1.8018e-01,  7.0776e-01, -2.2793e+00,\n",
      "        -9.4586e+00,  1.0423e+00, -1.7580e+00,  4.3831e-01, -2.8586e+00,\n",
      "         6.0574e-01,  5.8916e-01,  8.0252e-01, -3.7039e+00,  3.1795e-01,\n",
      "         3.9008e-01,  5.3908e-01, -2.6183e+00,  5.3387e-01, -1.5480e+00,\n",
      "        -5.6678e+00, -1.4061e+00,  1.8594e-01, -1.4946e+00, -4.3543e-01,\n",
      "         5.1890e-01, -8.4119e+00,  7.3733e-01,  1.0679e+00,  4.9158e-01,\n",
      "         1.8836e-01, -7.0114e+00, -3.0278e+00, -1.3281e+00,  4.2909e-01,\n",
      "         8.5751e-01, -1.4468e+00,  7.3465e-01, -6.7473e+00,  1.2675e+00,\n",
      "        -4.3278e+00,  9.9905e-01, -1.2550e+00, -5.1730e+00,  1.2735e-01,\n",
      "        -3.7170e-01, -4.1169e+00, -1.0491e+01, -5.2873e-01, -3.7603e+00,\n",
      "         7.4051e-01, -4.9359e+00,  6.2875e-01,  9.5923e-01, -5.5763e+00,\n",
      "        -5.2305e+00, -4.4583e+00, -8.1116e+00, -2.2269e+00, -1.5028e+00,\n",
      "        -2.7040e+00,  3.6900e-01,  7.0693e-01, -1.2221e+00,  6.5420e-02,\n",
      "        -1.6897e+00, -2.5653e+00,  1.1227e+00,  1.1092e+00,  9.9197e-01,\n",
      "         3.5951e-01, -4.9177e+00,  4.0430e-01,  1.4268e+00, -3.5834e+00,\n",
      "        -4.2169e+00, -6.0552e+00,  4.3582e-01,  8.6359e-01,  8.7728e-01,\n",
      "         1.0011e+00, -1.4094e+00, -6.9584e+00,  5.3328e-01, -1.6614e+00,\n",
      "         1.6467e-01,  7.2771e-01, -6.1414e+00,  6.7780e-01, -3.8377e+00,\n",
      "        -4.5165e+00, -4.0705e+00,  2.8830e-01, -7.6537e+00,  6.7260e-01,\n",
      "         7.5751e-01, -4.5142e+00,  5.9935e-01,  6.3376e-01, -1.0839e-01,\n",
      "         8.3061e-01, -5.2788e+00, -1.9927e+00,  2.4389e-01,  5.4436e-01,\n",
      "        -2.3003e+00,  2.4762e-02,  5.6520e-01, -3.4892e+00, -6.4351e+00,\n",
      "        -5.3591e+00, -7.0100e+00, -6.3621e+00, -5.2149e+00, -6.1532e-01,\n",
      "        -2.2147e+00,  8.7792e-01, -4.2384e+00,  7.5036e-01, -2.2993e+00,\n",
      "        -4.7125e+00,  9.8558e-02,  1.3864e+00, -5.2768e+00,  7.2802e-01,\n",
      "        -1.0180e+00,  2.3408e-01, -8.0267e+00, -3.5256e+00,  4.7946e-01,\n",
      "        -4.3450e+00, -2.2868e-01, -4.3897e+00,  1.1350e+00,  7.0034e-01,\n",
      "         4.4589e-01,  6.4241e-01,  6.3911e-01,  5.5767e-01,  3.5536e-01,\n",
      "        -1.5103e+00,  1.0515e+00, -1.4231e+00, -4.7862e+00, -1.6477e+00,\n",
      "        -2.6477e+00,  3.4442e-01, -6.0716e+00,  7.3650e-01,  1.3224e+00,\n",
      "        -1.2246e+00, -2.2051e-01,  5.5310e-01,  3.4425e-01, -1.3069e+00,\n",
      "        -8.4054e+00, -2.6260e+00, -9.1907e-02, -6.3040e+00, -1.0718e+01,\n",
      "        -2.1891e+00, -9.3300e+00,  7.9336e-01, -1.9036e+00, -1.5846e+00,\n",
      "        -1.4063e+00, -3.7143e+00, -7.2614e+00, -2.7321e-01,  9.9746e-01,\n",
      "         4.2256e-01,  1.1274e+00, -6.4217e+00,  1.3007e+00, -9.8688e-01,\n",
      "         5.3003e-01,  1.1396e+00, -4.0087e+00, -6.3932e-02,  1.0120e+00,\n",
      "         6.2221e-01,  5.7380e-01, -2.0724e+00,  8.5889e-01,  5.1576e-01,\n",
      "        -1.0136e+01, -4.2903e+00,  5.8732e-01, -4.9999e+00,  1.6785e-01,\n",
      "         3.4039e-01,  1.8489e-01,  2.7992e-01,  3.2022e-01, -1.8401e+00,\n",
      "        -1.6726e+00, -2.2410e+00,  6.3932e-01, -1.9079e+00,  7.2286e-01,\n",
      "        -3.5186e+00, -5.6702e-01,  1.9021e-01,  9.6147e-01,  3.9370e-01,\n",
      "        -3.4008e-01, -2.4742e+00,  3.6325e-01,  5.8325e-01,  8.5981e-01,\n",
      "        -4.0116e+00, -1.1031e+00, -2.4733e+00,  1.0106e+00, -1.0465e+01,\n",
      "        -3.5686e+00,  3.6788e-01, -3.8780e+00, -2.8643e+00, -7.2084e+00,\n",
      "         1.2384e-01, -1.9601e+00, -5.2394e+00, -4.4531e+00, -4.4070e+00,\n",
      "        -2.5423e-01,  2.9564e-01, -2.5333e+00,  3.3105e-01, -6.2216e+00,\n",
      "        -6.8223e-01,  8.4044e-01, -5.8673e+00, -3.5204e+00,  7.9393e-01,\n",
      "         4.5807e-01, -2.2164e+00, -4.2993e+00, -3.6709e+00,  6.2293e-01,\n",
      "        -4.0771e+00, -1.1370e+01,  3.5999e-01,  8.8093e-01, -1.5207e+00,\n",
      "         2.4865e-01,  3.4636e-01, -3.1709e+00, -5.5065e+00,  2.3180e-01,\n",
      "        -7.5148e-01,  6.0513e-01, -3.8261e+00, -3.5645e+00,  2.7765e-01,\n",
      "        -8.8928e-01,  9.8088e-01,  1.0049e+00,  1.1361e+00,  3.3315e-01,\n",
      "        -3.3178e+00,  1.3397e+00, -6.1929e+00, -3.6343e+00,  1.3030e-01,\n",
      "        -2.7604e+00, -6.2144e+00, -8.1769e-01, -5.3213e+00,  4.7384e-01,\n",
      "        -4.7324e+00, -1.0006e+00, -1.0706e+01, -5.0954e+00, -7.8826e+00,\n",
      "        -2.9185e+00,  1.0270e+00, -4.8312e+00, -1.1441e-01,  8.3909e-01,\n",
      "        -6.8376e-02,  1.0838e+00,  7.4336e-01, -5.7759e+00, -1.0985e+00,\n",
      "        -5.2198e+00, -2.1867e-01,  7.9441e-01, -1.1346e+01, -1.0049e+00,\n",
      "        -2.2145e+00, -4.1490e-01, -6.0172e+00,  1.1718e+00, -2.5759e+00,\n",
      "        -1.2873e+00, -5.9128e+00, -7.2567e+00, -7.3547e+00, -7.0051e-02,\n",
      "        -1.1184e+01,  1.1866e+00, -2.6198e+00,  2.9115e-01, -4.6188e+00,\n",
      "        -4.3439e+00, -8.3636e-02, -9.6385e+00, -7.8565e-01, -6.4592e+00,\n",
      "         9.6387e-01, -2.9763e+00,  3.9505e-01,  3.8396e-01, -4.6096e+00,\n",
      "        -2.6462e+00,  1.9291e-01,  1.1851e+00, -3.7046e+00,  7.2755e-01,\n",
      "        -1.8962e-01, -1.4723e+00, -1.3848e+00, -4.7293e+00, -1.1789e+00,\n",
      "        -3.5453e+00, -4.8027e+00,  4.9286e-02, -1.3840e+00, -3.9099e+00,\n",
      "         4.5131e-01,  2.5456e-01, -7.6135e+00, -5.2948e+00, -1.2305e+00,\n",
      "        -4.6639e+00, -6.5324e-01, -1.1003e+01,  2.4573e-01,  5.0474e-01,\n",
      "        -1.1416e+00,  2.6181e-01, -8.5386e+00, -5.8578e-01,  2.4868e-01,\n",
      "        -8.9469e-01, -8.8389e-02, -2.3268e-02,  9.8216e-01,  5.9790e-01,\n",
      "         1.0949e+00, -6.0294e-01, -2.8198e+00, -3.4534e+00, -2.2347e+00,\n",
      "        -3.2292e-01,  8.8836e-01, -6.6583e+00, -2.7017e+00,  2.7156e-01,\n",
      "         7.7775e-01, -1.1771e+01, -1.1263e+01, -1.8179e-01,  2.1122e-01,\n",
      "         6.6759e-01, -8.4930e+00,  9.8278e-01, -7.1932e-01,  3.4272e-01,\n",
      "        -8.0158e+00, -2.4995e+00, -9.8625e+00,  4.7697e-01, -1.4848e+00,\n",
      "        -3.0883e+00,  8.5427e-01,  1.0466e+00,  9.6497e-01,  1.2274e+00,\n",
      "         8.8767e-01,  7.0432e-01,  3.2232e-01,  6.8631e-01, -7.8403e-01,\n",
      "        -5.0594e+00,  1.0099e+00, -1.1133e+01, -7.2878e+00,  4.7595e-01,\n",
      "         1.1728e+00, -5.5774e-01, -5.1236e+00, -2.5827e+00, -3.5025e-01,\n",
      "        -8.2004e+00,  2.2057e-01, -2.8263e-01,  7.1353e-01,  1.0359e+00,\n",
      "         1.3983e-01,  2.9591e-01,  7.7497e-01, -1.8282e+00, -6.9326e-01,\n",
      "        -2.2758e+00, -6.6301e+00,  8.2859e-01, -1.8472e+00,  4.2412e-01,\n",
      "        -3.2727e+00,  4.8338e-01, -1.7171e+00, -3.3491e+00, -6.0940e+00,\n",
      "        -1.1999e+00, -2.3093e+00, -1.0473e+01, -4.7081e+00, -7.2189e+00,\n",
      "         1.0872e+00, -1.2620e+01,  1.2602e-01, -6.9416e+00, -2.1721e+00,\n",
      "        -4.6034e+00, -2.3011e+00, -4.6649e+00, -6.9534e-01,  1.0784e+00,\n",
      "        -1.1783e+01, -3.9752e+00, -8.9400e+00,  5.5233e-01, -6.5797e+00,\n",
      "         5.0112e-01,  4.2289e-01, -4.4253e+00,  4.6245e-01, -6.7931e+00,\n",
      "        -5.0770e+00,  9.9779e-01, -4.3808e+00,  7.5420e-01, -2.9799e+00,\n",
      "         5.4519e-01, -3.8617e+00, -3.0257e+00, -2.9562e-01,  6.6915e-01,\n",
      "         6.1710e-01,  3.3713e-01, -8.1173e+00,  2.3500e-03,  3.6070e-01,\n",
      "        -2.7272e+00, -6.1798e+00,  5.6991e-01, -1.2300e+00, -9.7702e-01,\n",
      "        -6.9814e-01, -7.0162e+00, -1.0831e+01,  7.2907e-01,  3.2294e-02,\n",
      "         7.4921e-01,  8.3854e-01, -1.6406e-02, -5.6755e+00, -4.3143e+00,\n",
      "        -8.5493e+00, -7.1584e+00, -2.8770e+00,  2.5846e-01,  9.2702e-01],\n",
      "       device='cuda:0') \n",
      "\n",
      "Accuracy:  0.8513621687889099\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy, spk_rec, mem_rec, spk_stack_hid, mem_stack_hid= measure_accuracy(snn, test_loader)\n",
    "print('spk_rec: ', spk_rec.shape,'\\n', spk_rec[24][0],'\\n')\n",
    "print('mem_rec: ', mem_rec.shape,'\\n',mem_rec[24][0],'\\n')\n",
    "print('spk_stack_hid: ', spk_stack_hid.shape,'\\n', spk_stack_hid[24][0],'\\n')\n",
    "print('mem_stack_hid: ', mem_stack_hid.shape,'\\n',mem_stack_hid[24][0],'\\n')\n",
    "print( 'Accuracy: ', accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
